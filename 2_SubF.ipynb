{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F89DFED-NHSV"
      },
      "source": [
        "# Learning Long Term Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioZR-i9rNHSW"
      },
      "source": [
        "There are p + 1 input symbols denoted a1, a2, . . . , ap−1, ap = x, ap+1 = y. ai\n",
        "is represented by p + 1 dimensional vector whose ith component is 1 and all\n",
        "other are 0. A net with p + 1 input units and p + 1 output units sequentially\n",
        "observes input symbol sequences, one at a time, trying to predict the next\n",
        "symbols. Error signals occur at every single time steps. To emphasize the\n",
        "long term lag problem, we use a training set consisting of only two sets of\n",
        "sequences: {(x, ai1, ai2, . . . , aip−1, x) | 1 ≤ i1 ≤ i2 ≤ . . . ≤ ip−1 ≤ p −1} and\n",
        "{(y, ai1, ai2, . . . , aip−1, y) |1 ≤i1 ≤i2 ≤. . . ≤ip−1 ≤p −1}. In this experiment\n",
        "take p = 100. The only totally predictable targets, however, are x and y, which\n",
        "occur at sequence ends. Training sequences are chosen randomly from the two\n",
        "sets with probability 0.5. Compare how RNN and LSTM perform for\n",
        "this prediction problem. Report the following.\n",
        "1. Describe the architecture used for LSTM and for RNN. Also mention the\n",
        "activation functions, optimizer and other parameters you choose. Experi-\n",
        "ment around with multiple architectures and report your observations.\n",
        "2. Plot the number of input sequences passed through the network versus\n",
        "training error (for both LSTM and RNN).\n",
        "3. Once the training stops, generate 3000 sequences for test set.\n",
        "4. Report the average number of wrong predictions on the test set in 10\n",
        "different trials (for both LSTM and RNN)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HMW-1HHaNHSX"
      },
      "outputs": [],
      "source": [
        "## Import Libraries\n",
        "# import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Fix a Random Seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# Use the GPU if available, otherwise stick with CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy1mp-INNHSZ"
      },
      "source": [
        "## Generating Your Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csv08BNFNHSZ",
        "outputId": "f07cba64-4a60-43d8-e568-3473c624c850"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/w9/n4f5mch54rs7r4cqyz2rxynm0000gp/T/ipykernel_48009/1017511607.py:60: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
            "  Train_x = torch.Tensor(Train_x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2000, 101, 101])\n",
            "torch.Size([2000, 101, 101])\n"
          ]
        }
      ],
      "source": [
        "# Generate the Two Data Points\n",
        "# a<i> = One hot vector with 1 at i-th position\n",
        "p = 100\n",
        "\n",
        "# This function generates a one hot vector with 1 at p-th position\n",
        "def one_hot_vector(p, n):\n",
        "    a = np.zeros(n)\n",
        "    a[p] = 1\n",
        "    return a\n",
        "\n",
        "def i_onehot(vec):\n",
        "    for i in range(len(vec)):\n",
        "        if vec[i] == 1:\n",
        "            return i\n",
        "\n",
        "def GenerateDatapoint(p):\n",
        "    x = one_hot_vector(p-1, p+1)\n",
        "    y = one_hot_vector(p, p+1)\n",
        "    x_point = []\n",
        "    y_point = []\n",
        "\n",
        "    x_point.append(x)\n",
        "    y_point.append(y)\n",
        "\n",
        "    x_point.append(one_hot_vector(np.random.randint(0, p-1), p+1))\n",
        "    # print(\"Index of One Hot for the First Term X\",i_onehot(x_point[1]))\n",
        "    y_point.append(one_hot_vector(np.random.randint(0, p-1), p+1))\n",
        "    # print(\"Index of One Hot for the First Term Y\", i_onehot(y_point[1]))\n",
        "\n",
        "    for i in range(2, p):\n",
        "        # Randomly choose a number the one hot index of the previous number, choose between one hot index of prev vector and p + 1\n",
        "        x_point.append(one_hot_vector(np.random.randint(i_onehot(x_point[i-1]), p-1), p+1))\n",
        "        y_point.append(one_hot_vector(np.random.randint(i_onehot(x_point[i-1]), p-1), p+1))\n",
        "\n",
        "    x_point.append(x)\n",
        "    y_point.append(y)\n",
        "\n",
        "    return x_point, y_point\n",
        "\n",
        "# Generate the Dataset\n",
        "def GenerateDataset(p, n):\n",
        "    x = []\n",
        "    y = []\n",
        "    for i in range(n):\n",
        "        x_point, y_point = GenerateDatapoint(p)\n",
        "        x.append(x_point)\n",
        "        y.append(y_point)\n",
        "    return x, y\n",
        "\n",
        "Dataset_x, Dataset_y = GenerateDataset(p, 2000)\n",
        "\n",
        "# Choose 1000 points for training and 1000 points for testing\n",
        "Train_x = Dataset_x[:1000]\n",
        "Train_y = Dataset_y[:1000]\n",
        "\n",
        "Test_x = Dataset_x[1000:]\n",
        "Test_y = Dataset_y[1000:]\n",
        "\n",
        "# Convert the Dataset into PyTorch Tensors\n",
        "Train_x = torch.Tensor(Train_x)\n",
        "Train_y = torch.Tensor(Train_y)\n",
        "\n",
        "Test_x = torch.Tensor(Test_x)\n",
        "Test_y = torch.Tensor(Test_y)\n",
        "\n",
        "Train = torch.cat((Train_x, Train_y), 0)\n",
        "Test = torch.cat((Test_x, Test_y), 0)\n",
        "\n",
        "# Shuffle the Dataset along dimension 0\n",
        "Train = Train[torch.randperm(Train.size()[0])]\n",
        "Test = Test[torch.randperm(Test.size()[0])]\n",
        "\n",
        "print(Train.shape)\n",
        "print(Test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3bKmk-uNHSZ"
      },
      "source": [
        "## Designing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fge4-CG6NHSa",
        "outputId": "97e3844c-57e9-4974-ed70-669c37b6ff7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2000, 101])\n"
          ]
        }
      ],
      "source": [
        "print(Train[:, 1, :].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LSTM Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class C_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, p):\n",
        "        super(C_LSTM, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.p = p\n",
        "\n",
        "        # Define the LSTM Layer\n",
        "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, train=True):\n",
        "        outputs = []\n",
        "        hidden_states = []\n",
        "        cell_states = []\n",
        "        self.initial_hidden = torch.rand(self.num_layers, x.shape[0], self.hidden_size)\n",
        "        self.initial_cell = torch.rand(self.num_layers, x.shape[0], self.hidden_size)\n",
        "        for timestep in range(self.p):\n",
        "            hidden_t = []\n",
        "            cell_t = []\n",
        "            if timestep == 0:\n",
        "                hidden_t, cell_t = self.lstm_cell(x[:, timestep], (self.initial_hidden[0], self.initial_cell[0]))\n",
        "            else:\n",
        "                input_t = []\n",
        "                if train:\n",
        "                    input_t = x[:, timestep]\n",
        "                else:\n",
        "                    input_t = torch.zeros_like(x[:, 0])\n",
        "                    temp = torch.argmax(outputs[-1], dim=1)\n",
        "                    for row in range(x.shape[0]):\n",
        "                        input_t[row][temp[row]] = 1.0\n",
        "                hidden_t, cell_t = self.lstm_cell(input_t, (hidden_states[-1], cell_states[-1]))\n",
        "            hidden_states.append(hidden_t)\n",
        "            cell_states.append(cell_t)\n",
        "            out = self.softmax(self.fc(hidden_t))\n",
        "            outputs.append(out)\n",
        "        return outputs[-1]\n",
        "\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "            return self.forward(x, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 4.55763\n",
            "Epoch: 10, loss: 0.70212\n",
            "Epoch: 20, loss: 0.69396\n",
            "Epoch: 30, loss: 0.69320\n",
            "Epoch: 40, loss: 0.69319\n",
            "Epoch: 50, loss: 0.69317\n",
            "Epoch: 60, loss: 0.69322\n",
            "Epoch: 70, loss: 0.69315\n",
            "Epoch: 80, loss: 0.69315\n",
            "Epoch: 90, loss: 0.69315\n",
            "Epoch: 100, loss: 0.69315\n",
            "Epoch: 110, loss: 0.69315\n",
            "Epoch: 120, loss: 0.69315\n",
            "Epoch: 130, loss: 0.69315\n",
            "Epoch: 140, loss: 0.69315\n",
            "Epoch: 150, loss: 0.69315\n",
            "Epoch: 160, loss: 0.69315\n",
            "Epoch: 170, loss: 0.69315\n",
            "Epoch: 180, loss: 0.69315\n",
            "Epoch: 190, loss: 0.69315\n",
            "Epoch: 200, loss: 0.69315\n",
            "Epoch: 210, loss: 0.69315\n",
            "Epoch: 220, loss: 0.69315\n",
            "Epoch: 230, loss: 0.69315\n",
            "Epoch: 240, loss: 0.69315\n",
            "Epoch: 250, loss: 0.69315\n",
            "Epoch: 260, loss: 0.69315\n",
            "Epoch: 270, loss: 0.69315\n",
            "Epoch: 280, loss: 0.69315\n",
            "Epoch: 290, loss: 0.69315\n",
            "Epoch: 300, loss: 0.69315\n",
            "Epoch: 310, loss: 0.69315\n",
            "Epoch: 320, loss: 0.69316\n",
            "Epoch: 330, loss: 0.70655\n",
            "Epoch: 340, loss: 0.69356\n",
            "Epoch: 350, loss: 0.69362\n",
            "Epoch: 360, loss: 0.69340\n",
            "Epoch: 370, loss: 0.69329\n",
            "Epoch: 380, loss: 0.69321\n",
            "Epoch: 390, loss: 0.69315\n",
            "Epoch: 400, loss: 0.69315\n",
            "Epoch: 410, loss: 0.69315\n",
            "Epoch: 420, loss: 0.69315\n",
            "Epoch: 430, loss: 0.69315\n",
            "Epoch: 440, loss: 0.69315\n",
            "Epoch: 450, loss: 0.69315\n",
            "Epoch: 460, loss: 0.69315\n",
            "Epoch: 470, loss: 0.69315\n",
            "Epoch: 480, loss: 0.69315\n",
            "Epoch: 490, loss: 0.69315\n",
            "Epoch: 500, loss: 0.69315\n",
            "Epoch: 510, loss: 0.69315\n",
            "Epoch: 520, loss: 0.69315\n",
            "Epoch: 530, loss: 0.69315\n",
            "Epoch: 540, loss: 0.69315\n",
            "Epoch: 550, loss: 0.69318\n",
            "Epoch: 560, loss: 0.70508\n",
            "Epoch: 570, loss: 0.69665\n",
            "Epoch: 580, loss: 0.69357\n",
            "Epoch: 590, loss: 0.69332\n",
            "Epoch: 600, loss: 0.69329\n",
            "Epoch: 610, loss: 0.69315\n",
            "Epoch: 620, loss: 0.69315\n",
            "Epoch: 630, loss: 0.69315\n",
            "Epoch: 640, loss: 0.69315\n",
            "Epoch: 650, loss: 0.69315\n",
            "Epoch: 660, loss: 0.69315\n",
            "Epoch: 670, loss: 0.69315\n",
            "Epoch: 680, loss: 0.69315\n",
            "Epoch: 690, loss: 0.69315\n",
            "Epoch: 700, loss: 0.69315\n",
            "Epoch: 710, loss: 0.69315\n",
            "Epoch: 720, loss: 0.69315\n",
            "Epoch: 730, loss: 0.69315\n",
            "Epoch: 740, loss: 0.69315\n",
            "Epoch: 750, loss: 0.69315\n",
            "Epoch: 760, loss: 0.69315\n",
            "Epoch: 770, loss: 0.69318\n",
            "Epoch: 780, loss: 0.70293\n",
            "Epoch: 790, loss: 0.69354\n",
            "Epoch: 800, loss: 0.69378\n",
            "Epoch: 810, loss: 0.69358\n",
            "Epoch: 820, loss: 0.69330\n",
            "Epoch: 830, loss: 0.69319\n",
            "Epoch: 840, loss: 0.69316\n",
            "Epoch: 850, loss: 0.69315\n",
            "Epoch: 860, loss: 0.69315\n",
            "Epoch: 870, loss: 0.69315\n",
            "Epoch: 880, loss: 0.69315\n",
            "Epoch: 890, loss: 0.69315\n",
            "Epoch: 900, loss: 0.69315\n",
            "Epoch: 910, loss: 0.69315\n",
            "Epoch: 920, loss: 0.69315\n",
            "Epoch: 930, loss: 0.69315\n",
            "Epoch: 940, loss: 0.69315\n",
            "Epoch: 950, loss: 0.69315\n",
            "Epoch: 960, loss: 0.69315\n",
            "Epoch: 970, loss: 0.69315\n",
            "Epoch: 980, loss: 0.69315\n",
            "Epoch: 990, loss: 0.69323\n",
            "Epoch: 1000, loss: 0.70284\n",
            "Epoch: 1010, loss: 0.69321\n",
            "Epoch: 1020, loss: 0.69318\n",
            "Epoch: 1030, loss: 0.69320\n"
          ]
        }
      ],
      "source": [
        "# Define the Hyperparameters\n",
        "n_hidden_layers = 2\n",
        "input_size = p+1\n",
        "hidden_size = 2*p+1\n",
        "output_size = p+1\n",
        "num_epochs = 500\n",
        "\n",
        "# Initialize the Model\n",
        "model_LSTM = C_LSTM(input_size, hidden_size, output_size, 1, p)\n",
        "# Run on the GPU\n",
        "model_LSTM.to(device)\n",
        "\n",
        "# Define the Loss Function and the Optimizer\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = optim.Adam(model_LSTM.parameters(), lr=0.001)\n",
        "\n",
        "loss_list = []\n",
        "test_loss_act = []\n",
        "test_loss_pred = []\n",
        "\n",
        "# Move Test tensor to the same device as the model\n",
        "Test = Test.to(device)\n",
        "\n",
        "# Make this batch training\n",
        "batch_size = \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    # Here, now you can call the Model\n",
        "    output = model_LSTM(Train)  # Pass the entire sequence to the model\n",
        "    # Calculate the loss for the Final Term only\n",
        "    loss = criterion(output, Train[:, -1, :]) # Last term in both\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_list.append(loss.item())\n",
        "    # test_loss_act.append(Loss_Test_Act(Test, model))\n",
        "    # test_loss_pred.append(Loss_Test_Pred(Test, model))\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
        "\n",
        "print(Train.shape)\n",
        "\n",
        "# Save the Model\n",
        "torch.save(model_LSTM.state_dict(), \"model.ckpt\")\n",
        "\n",
        "# Plot the Loss Curve\n",
        "plt.plot(loss_list)\n",
        "\n",
        "# Assuming test_loss_act and test_loss_pred are tensors on the GPU\n",
        "# Move them to the CPU before converting to NumPy arrays\n",
        "test_loss_act_cpu = [tl.detach().cpu().numpy() for tl in test_loss_act]\n",
        "test_loss_pred_cpu = [tl.detach().cpu().numpy() for tl in test_loss_pred]\n",
        "\n",
        "plt.plot(test_loss_act_cpu, color='red')\n",
        "plt.plot(test_loss_pred_cpu, color='green')\n",
        "\n",
        "# The rest of your code...\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the Predict Function on one test element\n",
        "# model.predict(0)\n",
        "i = 30\n",
        "print(Test[i, 0, :])\n",
        "pred = model_LSTM.predict(Test[i, :, :].unsqueeze(0))\n",
        "# Find the Maximum of the pred\n",
        "argmax = torch.argmax(pred, dim=1)\n",
        "final_result = torch.zeros_like(pred)\n",
        "final_result[0][argmax] = 1.0\n",
        "print(final_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9gnJqQ0hF09"
      },
      "source": [
        "## Write The Same Function for the RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "DW0CxA8ihF09",
        "outputId": "d766cc92-90f3-4e4c-e96a-a8620aea1e7c"
      },
      "outputs": [],
      "source": [
        "# Define the Hyperparameters\n",
        "n_hidden_layers = 2\n",
        "input_size = p+1\n",
        "hidden_size = 2*p+1\n",
        "output_size = p+1\n",
        "num_epochs = 2000\n",
        "\n",
        "# Define the RNN Class\n",
        "class C_RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, p):\n",
        "        super(C_RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.p = p\n",
        "\n",
        "        # Define the RNN Layer\n",
        "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.smax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    def forward(self, x, train = True):\n",
        "        outputs = []\n",
        "        hidden_states = []\n",
        "        self.initial_hidden = torch.rand(self.num_layers,x.shape[0],self.hidden_size)\n",
        "        for timestep in range(self.p):\n",
        "            hidden_t = []\n",
        "            if(timestep== 0):\n",
        "                hidden_t = self.rnn_cell(x[:,timestep], self.initial_hidden[0])\n",
        "            else:\n",
        "                input = []\n",
        "                if(train):\n",
        "                    input = x[:,timestep]\n",
        "                else:\n",
        "                    input = torch.zeros_like(x[:,0])\n",
        "                    temp = torch.argmax(outputs[-1],dim=1)\n",
        "                    for row in range(x.shape[0]):\n",
        "                        input[row][temp[row]] = 1.0\n",
        "                hidden_t = self.rnn_cell(input, hidden_states[-1])\n",
        "            hidden_states.append(hidden_t)\n",
        "            out = self.smax(self.fc(hidden_t))\n",
        "            outputs.append(out)\n",
        "        return outputs[-1]\n",
        "    \n",
        "    def predict(self, x):\n",
        "        return self.forward(x, False)\n",
        "\n",
        "# Initialize the Model\n",
        "model_RNN = C_RNN(input_size, hidden_size, output_size, 1, p)\n",
        "# Run on the GPU\n",
        "model_RNN.to(device)\n",
        "\n",
        "# Define the Loss Function and the Optimizer\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = optim.Adam(model_RNN.parameters(), lr=0.002)\n",
        "\n",
        "loss_list = []\n",
        "test_loss_act = []\n",
        "test_loss_pred = []\n",
        "\n",
        "# Move Test tensor to the same device as the model\n",
        "Test = Test.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    # Here, now you can call the Model\n",
        "    output = model_RNN(Train)  # Pass the entire sequence to the model\n",
        "    # Calculate the loss for the Final Term only\n",
        "    loss = criterion(output, Train[:, -1, :]) # Last term in both\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_list.append(loss.item())\n",
        "    # test_loss_act.append(Loss_Test_Act(Test, model))\n",
        "    # test_loss_pred.append(Loss_Test_Pred(Test, model))\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
        "\n",
        "print(Train.shape)\n",
        "\n",
        "# Save the Model\n",
        "torch.save(model_RNN.state_dict(), \"model.ckpt\")\n",
        "\n",
        "# Plot the Loss Curve\n",
        "plt.plot(loss_list)\n",
        "\n",
        "# Assuming test_loss_act and test_loss_pred are tensors on the GPU\n",
        "# Move them to the CPU before converting to NumPy arrays\n",
        "test_loss_act_cpu = [tl.detach().cpu().numpy() for tl in test_loss_act]\n",
        "test_loss_pred_cpu = [tl.detach().cpu().numpy() for tl in test_loss_pred]\n",
        "\n",
        "plt.plot(test_loss_act_cpu, color='red')\n",
        "plt.plot(test_loss_pred_cpu, color='green')\n",
        "\n",
        "# The rest of your code...\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the Predict Function on one test element\n",
        "# model.predict(0)\n",
        "i = 32\n",
        "print(Test[i, 0, :])\n",
        "pred = model_RNN.predict(Test[i, :, :].unsqueeze(0))\n",
        "# Find the Maximum of the pred\n",
        "argmax = torch.argmax(pred, dim=1)\n",
        "final_result = torch.zeros_like(pred)\n",
        "final_result[0][argmax] = 1.0\n",
        "print(final_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now, The Evaluation Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_models(model_LSTM, model_RNN, Test):\n",
        "    # Evaluate the LSTM\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    LSTM_Output = model_LSTM.predict(Test)\n",
        "\n",
        "    # Find the Argmax of the Output, as above\n",
        "    argmax = torch.argmax(LSTM_Output, dim=1)\n",
        "\n",
        "    # Find the number of correct predictions\n",
        "    correct = (argmax == Test[:, -1, :].argmax(dim=1)).sum().item()\n",
        "    total = Test.shape[0]\n",
        "\n",
        "    LSTM_Stat = (correct/total)\n",
        "\n",
        "    print(\"Accuracy of the LSTM on the Test Samples: %f\" % (correct/total))\n",
        "\n",
        "    # Evaluate the RNN\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    RNN_Output = model_RNN.predict(Test)\n",
        "\n",
        "    # Find the Argmax of the Output, as above\n",
        "    argmax = torch.argmax(RNN_Output, dim=1)\n",
        "\n",
        "    # Find the number of correct predictions\n",
        "    correct = (argmax == Test[:, -1, :].argmax(dim=1)).sum().item()\n",
        "    total = Test.shape[0]\n",
        "    print(\"Accuracy of the RNN on the Test Samples: %f\" % (correct/total))\n",
        "\n",
        "    RNN_Stat = (correct/total)\n",
        "    return LSTM_Stat, RNN_Stat\n",
        "\n",
        "# Generate Multiple Test Datasets similar to how generated above\n",
        "Datasets_Test = []\n",
        "for i in range(10):\n",
        "    Dataset_x, Dataset_y = GenerateDataset(p, 3000)\n",
        "    Test_x = Dataset_x[1500:]\n",
        "    Test_y = Dataset_y[1500:]\n",
        "\n",
        "    # Convert the Dataset into PyTorch Tensors\n",
        "    Test_x = torch.Tensor(Test_x)\n",
        "    Test_y = torch.Tensor(Test_y)\n",
        "\n",
        "    Datasets_Test.append(torch.cat((Test_x, Test_y), 0))\n",
        "\n",
        "# Move the Test Datasets to the same device as the model\n",
        "for i in range(len(Datasets_Test)):\n",
        "    Datasets_Test[i] = Datasets_Test[i].to(device)\n",
        "\n",
        "# Evaluate the Models on the Test Datasets\n",
        "LSTM_Stats = []\n",
        "RNN_Stats = []\n",
        "\n",
        "for i in range(len(Datasets_Test)):\n",
        "    LSTM_Stat, RNN_Stat = evaluate_models(model_LSTM, model_RNN, Datasets_Test[i])\n",
        "    LSTM_Stats.append(LSTM_Stat)\n",
        "    RNN_Stats.append(RNN_Stat)\n",
        "\n",
        "print(\"-------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"Average Accuracy of the LSTM on the Test Datasets: %f\" % (sum(LSTM_Stats)/len(LSTM_Stats)))\n",
        "print(\"Average Accuracy of the RNN on the Test Datasets: %f\" % (sum(RNN_Stats)/len(RNN_Stats)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
