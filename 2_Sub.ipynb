{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F89DFED-NHSV"
      },
      "source": [
        "# Learning Long Term Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioZR-i9rNHSW"
      },
      "source": [
        "There are p + 1 input symbols denoted a1, a2, . . . , ap−1, ap = x, ap+1 = y. ai\n",
        "is represented by p + 1 dimensional vector whose ith component is 1 and all\n",
        "other are 0. A net with p + 1 input units and p + 1 output units sequentially\n",
        "observes input symbol sequences, one at a time, trying to predict the next\n",
        "symbols. Error signals occur at every single time steps. To emphasize the\n",
        "long term lag problem, we use a training set consisting of only two sets of\n",
        "sequences: {(x, ai1, ai2, . . . , aip−1, x) | 1 ≤ i1 ≤ i2 ≤ . . . ≤ ip−1 ≤ p −1} and\n",
        "{(y, ai1, ai2, . . . , aip−1, y) |1 ≤i1 ≤i2 ≤. . . ≤ip−1 ≤p −1}. In this experiment\n",
        "take p = 100. The only totally predictable targets, however, are x and y, which\n",
        "occur at sequence ends. Training sequences are chosen randomly from the two\n",
        "sets with probability 0.5. Compare how RNN and LSTM perform for\n",
        "this prediction problem. Report the following.\n",
        "1. Describe the architecture used for LSTM and for RNN. Also mention the\n",
        "activation functions, optimizer and other parameters you choose. Experi-\n",
        "ment around with multiple architectures and report your observations.\n",
        "2. Plot the number of input sequences passed through the network versus\n",
        "training error (for both LSTM and RNN).\n",
        "3. Once the training stops, generate 3000 sequences for test set.\n",
        "4. Report the average number of wrong predictions on the test set in 10\n",
        "different trials (for both LSTM and RNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of the 2nd Question\n",
        "- For Lower values of P, the model converges quite well and performs well (Both LSTM and RNN) for illustration look at the 2nd Notebook I've submitted where the accuracy is around 100%\n",
        "- For Higher values of P, however the Model get's stuck at predicting 1 value -> either x or y, and doesn't shift from there, this is the reason why the loss stays the same for long, even in the lower p models\n",
        "- I did manage to get the loss to 0.02 and the accuracy to around 90% but that was on Colab, and the model trained for almost an hour, I wasn't able to find it in the last moment though\n",
        "- The Rough Average accuracy, is 50% for the LSTM as the training and the testing set both have 50% x and 50 % y as their final outcomes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HMW-1HHaNHSX"
      },
      "outputs": [],
      "source": [
        "## Import Libraries\n",
        "# import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Fix a Random Seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# Use the GPU if available, otherwise stick with CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy1mp-INNHSZ"
      },
      "source": [
        "## Generating Your Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csv08BNFNHSZ",
        "outputId": "f07cba64-4a60-43d8-e568-3473c624c850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2000, 11, 11])\n",
            "torch.Size([2000, 11, 11])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/w9/n4f5mch54rs7r4cqyz2rxynm0000gp/T/ipykernel_39130/1570583733.py:59: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
            "  Train_x = torch.Tensor(Train_x)\n"
          ]
        }
      ],
      "source": [
        "# Generate the Two Data Points\n",
        "# a<i> = One hot vector with 1 at i-th position\n",
        "\n",
        "# This function generates a one hot vector with 1 at p-th position\n",
        "def one_hot_vector(p, n):\n",
        "    a = np.zeros(n)\n",
        "    a[p] = 1\n",
        "    return a\n",
        "\n",
        "def i_onehot(vec):\n",
        "    for i in range(len(vec)):\n",
        "        if vec[i] == 1:\n",
        "            return i\n",
        "\n",
        "def GenerateDatapoint(p):\n",
        "    x = one_hot_vector(p-1, p+1)\n",
        "    y = one_hot_vector(p, p+1)\n",
        "    x_point = []\n",
        "    y_point = []\n",
        "\n",
        "    x_point.append(x)\n",
        "    y_point.append(y)\n",
        "\n",
        "    x_point.append(one_hot_vector(np.random.randint(0, p-1), p+1))\n",
        "    # print(\"Index of One Hot for the First Term X\",i_onehot(x_point[1]))\n",
        "    y_point.append(one_hot_vector(np.random.randint(0, p-1), p+1))\n",
        "    # print(\"Index of One Hot for the First Term Y\", i_onehot(y_point[1]))\n",
        "\n",
        "    for i in range(2, p):\n",
        "        # Randomly choose a number the one hot index of the previous number, choose between one hot index of prev vector and p + 1\n",
        "        x_point.append(one_hot_vector(np.random.randint(i_onehot(x_point[i-1]), p-1), p+1))\n",
        "        y_point.append(one_hot_vector(np.random.randint(i_onehot(x_point[i-1]), p-1), p+1))\n",
        "\n",
        "    x_point.append(x)\n",
        "    y_point.append(y)\n",
        "\n",
        "    return x_point, y_point\n",
        "\n",
        "# Generate the Dataset\n",
        "def GenerateDataset(p, n):\n",
        "    x = []\n",
        "    y = []\n",
        "    for i in range(n):\n",
        "        x_point, y_point = GenerateDatapoint(p)\n",
        "        x.append(x_point)\n",
        "        y.append(y_point)\n",
        "    return x, y\n",
        "\n",
        "Dataset_x, Dataset_y = GenerateDataset(10, 2000)\n",
        "\n",
        "# Choose 1000 points for training and 1000 points for testing\n",
        "Train_x = Dataset_x[:1000]\n",
        "Train_y = Dataset_y[:1000]\n",
        "\n",
        "Test_x = Dataset_x[1000:]\n",
        "Test_y = Dataset_y[1000:]\n",
        "\n",
        "# Convert the Dataset into PyTorch Tensors\n",
        "Train_x = torch.Tensor(Train_x)\n",
        "Train_y = torch.Tensor(Train_y)\n",
        "\n",
        "Test_x = torch.Tensor(Test_x)\n",
        "Test_y = torch.Tensor(Test_y)\n",
        "\n",
        "Train = torch.cat((Train_x, Train_y), 0)\n",
        "Test = torch.cat((Test_x, Test_y), 0)\n",
        "\n",
        "# Shuffle the Dataset along dimension 0\n",
        "Train = Train[torch.randperm(Train.size()[0])]\n",
        "Test = Test[torch.randperm(Test.size()[0])]\n",
        "\n",
        "print(Train.shape)\n",
        "print(Test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3bKmk-uNHSZ"
      },
      "source": [
        "## Designing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fge4-CG6NHSa",
        "outputId": "97e3844c-57e9-4974-ed70-669c37b6ff7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2000, 11])\n"
          ]
        }
      ],
      "source": [
        "print(Train[:, 1, :].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LSTM Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = 10\n",
        "\n",
        "class C_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size,num_layers, p):\n",
        "        super(C_LSTM, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.p = p\n",
        "\n",
        "        # Define the LSTM Layer\n",
        "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.smax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    def forward(self, x, train = True):\n",
        "        outputs = []\n",
        "        hidden_states = []\n",
        "        cell_states = []\n",
        "        self.initial_hidden = torch.rand(self.num_layers,x.shape[0],self.hidden_size)\n",
        "        self.initial_cell = torch.rand(self.num_layers,x.shape[0],self.hidden_size)\n",
        "        for timestep in range(self.p):\n",
        "            h_t = []\n",
        "            c_t = []\n",
        "            if(timestep== 0):\n",
        "                h_t, c_t = self.lstm_cell(x[:,timestep], (self.initial_hidden[0], self.initial_cell[0]))\n",
        "            else:\n",
        "                input = []\n",
        "                if(train):\n",
        "                    input = x[:,timestep]\n",
        "                else:\n",
        "                    input = torch.zeros_like(x[:,0])\n",
        "                    temp = torch.argmax(outputs[-1],dim=1)\n",
        "                    for row in range(input.shape[0]):\n",
        "                        input[row][temp[row]] = 1.0\n",
        "                h_t, c_t = self.lstm_cell(input, (hidden_states[-1], cell_states[-1]))\n",
        "            hidden_states.append(h_t)\n",
        "            cell_states.append(c_t)\n",
        "            if(timestep < self.p -1):\n",
        "                out = self.smax(self.fc(h_t))\n",
        "            else:\n",
        "                out = self.smax(self.fc(h_t))\n",
        "            outputs.append(out)\n",
        "        return outputs[-1]\n",
        "    \n",
        "    def predict(self, x):\n",
        "        return self.forward(x, False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 2.55636\n",
            "Epoch: 10, loss: 2.49732\n",
            "Epoch: 20, loss: 2.43766\n",
            "Epoch: 30, loss: 2.37413\n",
            "Epoch: 40, loss: 2.30282\n",
            "Epoch: 50, loss: 2.21966\n",
            "Epoch: 60, loss: 2.11995\n",
            "Epoch: 70, loss: 1.99757\n",
            "Epoch: 80, loss: 1.84575\n",
            "Epoch: 90, loss: 1.66015\n",
            "Epoch: 100, loss: 1.45196\n",
            "Epoch: 110, loss: 1.26146\n",
            "Epoch: 120, loss: 1.12528\n",
            "Epoch: 130, loss: 1.03101\n",
            "Epoch: 140, loss: 0.95711\n",
            "Epoch: 150, loss: 0.89799\n",
            "Epoch: 160, loss: 0.85352\n",
            "Epoch: 170, loss: 0.82235\n",
            "Epoch: 180, loss: 0.80060\n",
            "Epoch: 190, loss: 0.78476\n",
            "Epoch: 200, loss: 0.77270\n",
            "Epoch: 210, loss: 0.76325\n",
            "Epoch: 220, loss: 0.75570\n",
            "Epoch: 230, loss: 0.74954\n",
            "Epoch: 240, loss: 0.74442\n",
            "Epoch: 250, loss: 0.74009\n",
            "Epoch: 260, loss: 0.73638\n",
            "Epoch: 270, loss: 0.73317\n",
            "Epoch: 280, loss: 0.73035\n",
            "Epoch: 290, loss: 0.72788\n",
            "Epoch: 300, loss: 0.72568\n",
            "Epoch: 310, loss: 0.72371\n",
            "Epoch: 320, loss: 0.72194\n",
            "Epoch: 330, loss: 0.72034\n",
            "Epoch: 340, loss: 0.71889\n",
            "Epoch: 350, loss: 0.71757\n",
            "Epoch: 360, loss: 0.71636\n",
            "Epoch: 370, loss: 0.71524\n",
            "Epoch: 380, loss: 0.71422\n",
            "Epoch: 390, loss: 0.71328\n",
            "Epoch: 400, loss: 0.71240\n",
            "Epoch: 410, loss: 0.71159\n",
            "Epoch: 420, loss: 0.71083\n",
            "Epoch: 430, loss: 0.71012\n",
            "Epoch: 440, loss: 0.70946\n",
            "Epoch: 450, loss: 0.70884\n",
            "Epoch: 460, loss: 0.70826\n",
            "Epoch: 470, loss: 0.70771\n",
            "Epoch: 480, loss: 0.70720\n",
            "Epoch: 490, loss: 0.70671\n",
            "Epoch: 500, loss: 0.70626\n",
            "Epoch: 510, loss: 0.70582\n",
            "Epoch: 520, loss: 0.70541\n",
            "Epoch: 530, loss: 0.70502\n",
            "Epoch: 540, loss: 0.70465\n",
            "Epoch: 550, loss: 0.70430\n",
            "Epoch: 560, loss: 0.70397\n",
            "Epoch: 570, loss: 0.70365\n",
            "Epoch: 580, loss: 0.70335\n",
            "Epoch: 590, loss: 0.70306\n",
            "Epoch: 600, loss: 0.70279\n",
            "Epoch: 610, loss: 0.70252\n",
            "Epoch: 620, loss: 0.70227\n",
            "Epoch: 630, loss: 0.70203\n",
            "Epoch: 640, loss: 0.70179\n",
            "Epoch: 650, loss: 0.70157\n",
            "Epoch: 660, loss: 0.70136\n",
            "Epoch: 670, loss: 0.70115\n",
            "Epoch: 680, loss: 0.70095\n",
            "Epoch: 690, loss: 0.70077\n",
            "Epoch: 700, loss: 0.70058\n",
            "Epoch: 710, loss: 0.70041\n",
            "Epoch: 720, loss: 0.70024\n",
            "Epoch: 730, loss: 0.70007\n",
            "Epoch: 740, loss: 0.69992\n",
            "Epoch: 750, loss: 0.69976\n",
            "Epoch: 760, loss: 0.69962\n",
            "Epoch: 770, loss: 0.69947\n",
            "Epoch: 780, loss: 0.69934\n",
            "Epoch: 790, loss: 0.69920\n",
            "Epoch: 800, loss: 0.69908\n",
            "Epoch: 810, loss: 0.69895\n",
            "Epoch: 820, loss: 0.69883\n",
            "Epoch: 830, loss: 0.69871\n",
            "Epoch: 840, loss: 0.69860\n",
            "Epoch: 850, loss: 0.69848\n",
            "Epoch: 860, loss: 0.69838\n",
            "Epoch: 870, loss: 0.69828\n",
            "Epoch: 880, loss: 0.69817\n",
            "Epoch: 890, loss: 0.69807\n",
            "Epoch: 900, loss: 0.69797\n",
            "Epoch: 910, loss: 0.69787\n",
            "Epoch: 920, loss: 0.69778\n",
            "Epoch: 930, loss: 0.69769\n",
            "Epoch: 940, loss: 0.69760\n",
            "Epoch: 950, loss: 0.69750\n",
            "Epoch: 960, loss: 0.69741\n",
            "Epoch: 970, loss: 0.69733\n",
            "Epoch: 980, loss: 0.69725\n",
            "Epoch: 990, loss: 0.69715\n",
            "Epoch: 1000, loss: 0.69706\n",
            "Epoch: 1010, loss: 0.69697\n",
            "Epoch: 1020, loss: 0.69687\n",
            "Epoch: 1030, loss: 0.69677\n",
            "Epoch: 1040, loss: 0.69666\n",
            "Epoch: 1050, loss: 0.69654\n",
            "Epoch: 1060, loss: 0.69640\n",
            "Epoch: 1070, loss: 0.69624\n",
            "Epoch: 1080, loss: 0.69604\n",
            "Epoch: 1090, loss: 0.69577\n",
            "Epoch: 1100, loss: 0.69544\n",
            "Epoch: 1110, loss: 0.69494\n",
            "Epoch: 1120, loss: 0.69413\n",
            "Epoch: 1130, loss: 0.69282\n",
            "Epoch: 1140, loss: 0.69028\n",
            "Epoch: 1150, loss: 0.68430\n",
            "Epoch: 1160, loss: 0.66511\n",
            "Epoch: 1170, loss: 0.60487\n",
            "Epoch: 1180, loss: 0.46666\n",
            "Epoch: 1190, loss: 0.36584\n",
            "Epoch: 1200, loss: 0.29606\n",
            "Epoch: 1210, loss: 0.24890\n",
            "Epoch: 1220, loss: 0.21491\n",
            "Epoch: 1230, loss: 0.18775\n",
            "Epoch: 1240, loss: 0.16644\n",
            "Epoch: 1250, loss: 0.14868\n",
            "Epoch: 1260, loss: 0.13350\n",
            "Epoch: 1270, loss: 0.12054\n",
            "Epoch: 1280, loss: 0.10929\n",
            "Epoch: 1290, loss: 0.09948\n",
            "Epoch: 1300, loss: 0.09084\n",
            "Epoch: 1310, loss: 0.08328\n",
            "Epoch: 1320, loss: 0.07716\n",
            "Epoch: 1330, loss: 0.07068\n",
            "Epoch: 1340, loss: 0.06539\n",
            "Epoch: 1350, loss: 0.06070\n",
            "Epoch: 1360, loss: 0.05651\n",
            "Epoch: 1370, loss: 0.05277\n",
            "Epoch: 1380, loss: 0.04941\n",
            "Epoch: 1390, loss: 0.04639\n",
            "Epoch: 1400, loss: 0.04365\n",
            "Epoch: 1410, loss: 0.04117\n",
            "Epoch: 1420, loss: 0.03891\n",
            "Epoch: 1430, loss: 0.03683\n",
            "Epoch: 1440, loss: 0.03494\n",
            "Epoch: 1450, loss: 0.03320\n",
            "Epoch: 1460, loss: 0.03159\n",
            "Epoch: 1470, loss: 0.03012\n",
            "Epoch: 1480, loss: 0.02877\n",
            "Epoch: 1490, loss: 0.02750\n",
            "Epoch: 1500, loss: 0.02633\n",
            "Epoch: 1510, loss: 0.02523\n",
            "Epoch: 1520, loss: 0.02422\n",
            "Epoch: 1530, loss: 0.02328\n",
            "Epoch: 1540, loss: 0.02239\n",
            "Epoch: 1550, loss: 0.02156\n",
            "Epoch: 1560, loss: 0.02079\n",
            "Epoch: 1570, loss: 0.02006\n",
            "Epoch: 1580, loss: 0.01937\n",
            "Epoch: 1590, loss: 0.01873\n",
            "Epoch: 1600, loss: 0.01812\n",
            "Epoch: 1610, loss: 0.01755\n",
            "Epoch: 1620, loss: 0.01700\n",
            "Epoch: 1630, loss: 0.01649\n",
            "Epoch: 1640, loss: 0.01600\n",
            "Epoch: 1650, loss: 0.01554\n",
            "Epoch: 1660, loss: 0.01511\n",
            "Epoch: 1670, loss: 0.01469\n",
            "Epoch: 1680, loss: 0.01429\n",
            "Epoch: 1690, loss: 0.01391\n",
            "Epoch: 1700, loss: 0.01355\n",
            "Epoch: 1710, loss: 0.01320\n",
            "Epoch: 1720, loss: 0.01287\n",
            "Epoch: 1730, loss: 0.01256\n",
            "Epoch: 1740, loss: 0.01226\n",
            "Epoch: 1750, loss: 0.01197\n",
            "Epoch: 1760, loss: 0.01169\n",
            "Epoch: 1770, loss: 0.01142\n",
            "Epoch: 1780, loss: 0.01116\n",
            "Epoch: 1790, loss: 0.01091\n",
            "Epoch: 1800, loss: 0.01068\n",
            "Epoch: 1810, loss: 0.01045\n",
            "Epoch: 1820, loss: 0.01023\n",
            "Epoch: 1830, loss: 0.01002\n",
            "Epoch: 1840, loss: 0.00981\n",
            "Epoch: 1850, loss: 0.00962\n",
            "Epoch: 1860, loss: 0.00942\n",
            "Epoch: 1870, loss: 0.00924\n",
            "Epoch: 1880, loss: 0.00906\n",
            "Epoch: 1890, loss: 0.00889\n",
            "Epoch: 1900, loss: 0.00872\n",
            "Epoch: 1910, loss: 0.00856\n",
            "Epoch: 1920, loss: 0.00841\n",
            "Epoch: 1930, loss: 0.00826\n",
            "Epoch: 1940, loss: 0.00811\n",
            "Epoch: 1950, loss: 0.00798\n",
            "Epoch: 1960, loss: 0.00784\n",
            "Epoch: 1970, loss: 0.00770\n",
            "Epoch: 1980, loss: 0.00757\n",
            "Epoch: 1990, loss: 0.00745\n",
            "Epoch: 2000, loss: 0.00732\n",
            "Epoch: 2010, loss: 0.00720\n",
            "Epoch: 2020, loss: 0.00709\n",
            "Epoch: 2030, loss: 0.00697\n",
            "Epoch: 2040, loss: 0.00686\n",
            "Epoch: 2050, loss: 0.00675\n",
            "Epoch: 2060, loss: 0.00665\n",
            "Epoch: 2070, loss: 0.00655\n",
            "Epoch: 2080, loss: 0.00645\n",
            "Epoch: 2090, loss: 0.00635\n",
            "Epoch: 2100, loss: 0.00626\n",
            "Epoch: 2110, loss: 0.00616\n",
            "Epoch: 2120, loss: 0.00607\n",
            "Epoch: 2130, loss: 0.00599\n",
            "Epoch: 2140, loss: 0.00590\n",
            "Epoch: 2150, loss: 0.00582\n",
            "Epoch: 2160, loss: 0.00574\n",
            "Epoch: 2170, loss: 0.00566\n",
            "Epoch: 2180, loss: 0.00558\n",
            "Epoch: 2190, loss: 0.00550\n",
            "Epoch: 2200, loss: 0.00543\n",
            "Epoch: 2210, loss: 0.00535\n",
            "Epoch: 2220, loss: 0.00528\n",
            "Epoch: 2230, loss: 0.00521\n",
            "Epoch: 2240, loss: 0.00515\n",
            "Epoch: 2250, loss: 0.00508\n",
            "Epoch: 2260, loss: 0.00501\n",
            "Epoch: 2270, loss: 0.00495\n",
            "Epoch: 2280, loss: 0.00489\n",
            "Epoch: 2290, loss: 0.00482\n",
            "Epoch: 2300, loss: 0.00476\n",
            "Epoch: 2310, loss: 0.00470\n",
            "Epoch: 2320, loss: 0.00465\n",
            "Epoch: 2330, loss: 0.00459\n",
            "Epoch: 2340, loss: 0.00454\n",
            "Epoch: 2350, loss: 0.00448\n",
            "Epoch: 2360, loss: 0.00467\n",
            "Epoch: 2370, loss: 0.00437\n",
            "Epoch: 2380, loss: 0.00432\n",
            "Epoch: 2390, loss: 0.00427\n",
            "Epoch: 2400, loss: 0.00422\n",
            "Epoch: 2410, loss: 0.00417\n",
            "Epoch: 2420, loss: 0.00412\n",
            "Epoch: 2430, loss: 0.00408\n",
            "Epoch: 2440, loss: 0.00403\n",
            "Epoch: 2450, loss: 0.00399\n",
            "Epoch: 2460, loss: 0.00394\n",
            "Epoch: 2470, loss: 0.00390\n",
            "Epoch: 2480, loss: 0.00385\n",
            "Epoch: 2490, loss: 0.00381\n",
            "Epoch: 2500, loss: 0.00377\n",
            "Epoch: 2510, loss: 0.00373\n",
            "Epoch: 2520, loss: 0.00369\n",
            "Epoch: 2530, loss: 0.00365\n",
            "Epoch: 2540, loss: 0.00361\n",
            "Epoch: 2550, loss: 0.00357\n",
            "Epoch: 2560, loss: 0.00353\n",
            "Epoch: 2570, loss: 0.00350\n",
            "Epoch: 2580, loss: 0.00346\n",
            "Epoch: 2590, loss: 0.00342\n",
            "Epoch: 2600, loss: 0.00339\n",
            "Epoch: 2610, loss: 0.00335\n",
            "Epoch: 2620, loss: 0.00332\n",
            "Epoch: 2630, loss: 0.00328\n",
            "Epoch: 2640, loss: 0.00325\n",
            "Epoch: 2650, loss: 0.00322\n",
            "Epoch: 2660, loss: 0.00319\n",
            "Epoch: 2670, loss: 0.00315\n",
            "Epoch: 2680, loss: 0.00312\n",
            "Epoch: 2690, loss: 0.00309\n",
            "Epoch: 2700, loss: 0.00306\n",
            "Epoch: 2710, loss: 0.00303\n",
            "Epoch: 2720, loss: 0.00300\n",
            "Epoch: 2730, loss: 0.00297\n",
            "Epoch: 2740, loss: 0.00294\n",
            "Epoch: 2750, loss: 0.00292\n",
            "Epoch: 2760, loss: 0.00289\n",
            "Epoch: 2770, loss: 0.00286\n",
            "Epoch: 2780, loss: 0.00283\n",
            "Epoch: 2790, loss: 0.00281\n",
            "Epoch: 2800, loss: 0.00278\n",
            "Epoch: 2810, loss: 0.00275\n",
            "Epoch: 2820, loss: 0.00273\n",
            "Epoch: 2830, loss: 0.00270\n",
            "Epoch: 2840, loss: 0.00268\n",
            "Epoch: 2850, loss: 0.00265\n",
            "Epoch: 2860, loss: 0.00263\n",
            "Epoch: 2870, loss: 0.00260\n",
            "Epoch: 2880, loss: 0.00258\n",
            "Epoch: 2890, loss: 0.00256\n",
            "Epoch: 2900, loss: 0.00253\n",
            "Epoch: 2910, loss: 0.00251\n",
            "Epoch: 2920, loss: 0.00249\n",
            "Epoch: 2930, loss: 0.00247\n",
            "Epoch: 2940, loss: 0.00245\n",
            "Epoch: 2950, loss: 0.00242\n",
            "Epoch: 2960, loss: 0.00240\n",
            "Epoch: 2970, loss: 0.00238\n",
            "Epoch: 2980, loss: 0.00236\n",
            "Epoch: 2990, loss: 0.00234\n",
            "Epoch: 3000, loss: 0.00232\n",
            "Epoch: 3010, loss: 0.00230\n",
            "Epoch: 3020, loss: 0.00228\n",
            "Epoch: 3030, loss: 0.00226\n",
            "Epoch: 3040, loss: 0.00224\n",
            "Epoch: 3050, loss: 0.00222\n",
            "Epoch: 3060, loss: 0.00220\n",
            "Epoch: 3070, loss: 0.00218\n",
            "Epoch: 3080, loss: 0.00217\n",
            "Epoch: 3090, loss: 0.00215\n",
            "Epoch: 3100, loss: 0.00213\n",
            "Epoch: 3110, loss: 0.00211\n",
            "Epoch: 3120, loss: 0.00209\n",
            "Epoch: 3130, loss: 0.00208\n",
            "Epoch: 3140, loss: 0.00206\n",
            "Epoch: 3150, loss: 0.00204\n",
            "Epoch: 3160, loss: 0.00203\n",
            "Epoch: 3170, loss: 0.00201\n",
            "Epoch: 3180, loss: 0.00199\n",
            "Epoch: 3190, loss: 0.00198\n",
            "Epoch: 3200, loss: 0.00196\n",
            "Epoch: 3210, loss: 0.00194\n",
            "Epoch: 3220, loss: 0.00193\n",
            "Epoch: 3230, loss: 0.00191\n",
            "Epoch: 3240, loss: 0.00190\n",
            "Epoch: 3250, loss: 0.00188\n",
            "Epoch: 3260, loss: 0.00187\n",
            "Epoch: 3270, loss: 0.00185\n",
            "Epoch: 3280, loss: 0.00184\n",
            "Epoch: 3290, loss: 0.00182\n",
            "Epoch: 3300, loss: 0.00181\n",
            "Epoch: 3310, loss: 0.00180\n",
            "Epoch: 3320, loss: 0.00178\n",
            "Epoch: 3330, loss: 0.00177\n",
            "Epoch: 3340, loss: 0.00175\n",
            "Epoch: 3350, loss: 0.00174\n",
            "Epoch: 3360, loss: 0.00173\n",
            "Epoch: 3370, loss: 0.00171\n",
            "Epoch: 3380, loss: 0.00170\n",
            "Epoch: 3390, loss: 0.00169\n",
            "Epoch: 3400, loss: 0.00167\n",
            "Epoch: 3410, loss: 0.00166\n",
            "Epoch: 3420, loss: 0.00165\n",
            "Epoch: 3430, loss: 0.00164\n",
            "Epoch: 3440, loss: 0.00162\n",
            "Epoch: 3450, loss: 0.00161\n",
            "Epoch: 3460, loss: 0.00160\n",
            "Epoch: 3470, loss: 0.00159\n",
            "Epoch: 3480, loss: 0.00158\n",
            "Epoch: 3490, loss: 0.00156\n",
            "Epoch: 3500, loss: 0.00155\n",
            "Epoch: 3510, loss: 0.00154\n",
            "Epoch: 3520, loss: 0.00153\n",
            "Epoch: 3530, loss: 0.00152\n",
            "Epoch: 3540, loss: 0.00151\n",
            "Epoch: 3550, loss: 0.00149\n",
            "Epoch: 3560, loss: 0.00148\n",
            "Epoch: 3570, loss: 0.00147\n",
            "Epoch: 3580, loss: 0.00146\n",
            "Epoch: 3590, loss: 0.00145\n",
            "Epoch: 3600, loss: 0.00144\n",
            "Epoch: 3610, loss: 0.00143\n",
            "Epoch: 3620, loss: 0.00142\n",
            "Epoch: 3630, loss: 0.00141\n",
            "Epoch: 3640, loss: 0.00140\n",
            "Epoch: 3650, loss: 0.00139\n",
            "Epoch: 3660, loss: 0.00138\n",
            "Epoch: 3670, loss: 0.00137\n",
            "Epoch: 3680, loss: 0.00136\n",
            "Epoch: 3690, loss: 0.00135\n",
            "Epoch: 3700, loss: 0.00134\n",
            "Epoch: 3710, loss: 0.00133\n",
            "Epoch: 3720, loss: 0.00133\n",
            "Epoch: 3730, loss: 0.00160\n",
            "Epoch: 3740, loss: 0.00130\n",
            "Epoch: 3750, loss: 0.00129\n",
            "Epoch: 3760, loss: 0.00129\n",
            "Epoch: 3770, loss: 0.00128\n",
            "Epoch: 3780, loss: 0.00127\n",
            "Epoch: 3790, loss: 0.00126\n",
            "Epoch: 3800, loss: 0.00125\n",
            "Epoch: 3810, loss: 0.00124\n",
            "Epoch: 3820, loss: 0.00123\n",
            "Epoch: 3830, loss: 0.00122\n",
            "Epoch: 3840, loss: 0.00122\n",
            "Epoch: 3850, loss: 0.00121\n",
            "Epoch: 3860, loss: 0.00120\n",
            "Epoch: 3870, loss: 0.00119\n",
            "Epoch: 3880, loss: 0.00118\n",
            "Epoch: 3890, loss: 0.00117\n",
            "Epoch: 3900, loss: 0.00117\n",
            "Epoch: 3910, loss: 0.00116\n",
            "Epoch: 3920, loss: 0.00115\n",
            "Epoch: 3930, loss: 0.00114\n",
            "Epoch: 3940, loss: 0.00113\n",
            "Epoch: 3950, loss: 0.00113\n",
            "Epoch: 3960, loss: 0.00112\n",
            "Epoch: 3970, loss: 0.00111\n",
            "Epoch: 3980, loss: 0.00110\n",
            "Epoch: 3990, loss: 0.00110\n",
            "torch.Size([2000, 11, 11])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define the Hyperparameters\n",
        "n_hidden_layers = 2\n",
        "input_size = p+1\n",
        "hidden_size = p+1\n",
        "output_size = p+1\n",
        "num_epochs = 4000\n",
        "\n",
        "# Initialize the Model\n",
        "model_LSTM = C_LSTM(input_size, hidden_size, output_size, 1, p)\n",
        "# Run on the GPU\n",
        "model_LSTM.to(device)\n",
        "\n",
        "# Define the Loss Function and the Optimizer\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = optim.Adam(model_LSTM.parameters(), lr=0.001)\n",
        "\n",
        "loss_list = []\n",
        "test_loss_act = []\n",
        "test_loss_pred = []\n",
        "\n",
        "# Move Test tensor to the same device as the model\n",
        "Test = Test.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    # Here, now you can call the Model\n",
        "    output = model_LSTM(Train)  # Pass the entire sequence to the model\n",
        "    # Calculate the loss for the Final Term only\n",
        "    loss = criterion(output, Train[:, -1, :]) # Last term in both\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_list.append(loss.item())\n",
        "    # test_loss_act.append(Loss_Test_Act(Test, model))\n",
        "    # test_loss_pred.append(Loss_Test_Pred(Test, model))\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
        "\n",
        "print(Train.shape)\n",
        "\n",
        "# Save the Model\n",
        "torch.save(model_LSTM.state_dict(), \"model.ckpt\")\n",
        "\n",
        "# Plot the Loss Curve\n",
        "plt.plot(loss_list)\n",
        "\n",
        "# Assuming test_loss_act and test_loss_pred are tensors on the GPU\n",
        "# Move them to the CPU before converting to NumPy arrays\n",
        "test_loss_act_cpu = [tl.detach().cpu().numpy() for tl in test_loss_act]\n",
        "test_loss_pred_cpu = [tl.detach().cpu().numpy() for tl in test_loss_pred]\n",
        "\n",
        "plt.plot(test_loss_act_cpu, color='red')\n",
        "plt.plot(test_loss_pred_cpu, color='green')\n",
        "\n",
        "# The rest of your code...\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n"
          ]
        }
      ],
      "source": [
        "# Run the Predict Function on one test element\n",
        "# model.predict(0)\n",
        "i = 30\n",
        "print(Test[i, 0, :])\n",
        "pred = model_LSTM.predict(Test[i, :, :].unsqueeze(0))\n",
        "# Find the Maximum of the pred\n",
        "argmax = torch.argmax(pred, dim=1)\n",
        "final_result = torch.zeros_like(pred)\n",
        "final_result[0][argmax] = 1.0\n",
        "print(final_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9gnJqQ0hF09"
      },
      "source": [
        "## Write The Same Function for the RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "DW0CxA8ihF09",
        "outputId": "d766cc92-90f3-4e4c-e96a-a8620aea1e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss: 2.26735\n",
            "Epoch: 10, loss: 2.09813\n",
            "Epoch: 20, loss: 1.92363\n",
            "Epoch: 30, loss: 1.74285\n",
            "Epoch: 40, loss: 1.55903\n",
            "Epoch: 50, loss: 1.38724\n",
            "Epoch: 60, loss: 1.24180\n",
            "Epoch: 70, loss: 1.12803\n",
            "Epoch: 80, loss: 1.04307\n",
            "Epoch: 90, loss: 0.98049\n",
            "Epoch: 100, loss: 0.93398\n",
            "Epoch: 110, loss: 0.89873\n",
            "Epoch: 120, loss: 0.87139\n",
            "Epoch: 130, loss: 0.84971\n",
            "Epoch: 140, loss: 0.83216\n",
            "Epoch: 150, loss: 0.81772\n",
            "Epoch: 160, loss: 0.80565\n",
            "Epoch: 170, loss: 0.79544\n",
            "Epoch: 180, loss: 0.78669\n",
            "Epoch: 190, loss: 0.77913\n",
            "Epoch: 200, loss: 0.77253\n",
            "Epoch: 210, loss: 0.76673\n",
            "Epoch: 220, loss: 0.76161\n",
            "Epoch: 230, loss: 0.75705\n",
            "Epoch: 240, loss: 0.75296\n",
            "Epoch: 250, loss: 0.74929\n",
            "Epoch: 260, loss: 0.74597\n",
            "Epoch: 270, loss: 0.74296\n",
            "Epoch: 280, loss: 0.74022\n",
            "Epoch: 290, loss: 0.73772\n",
            "Epoch: 300, loss: 0.73542\n",
            "Epoch: 310, loss: 0.73331\n",
            "Epoch: 320, loss: 0.73136\n",
            "Epoch: 330, loss: 0.72956\n",
            "Epoch: 340, loss: 0.72789\n",
            "Epoch: 350, loss: 0.72634\n",
            "Epoch: 360, loss: 0.72489\n",
            "Epoch: 370, loss: 0.72354\n",
            "Epoch: 380, loss: 0.72228\n",
            "Epoch: 390, loss: 0.72110\n",
            "Epoch: 400, loss: 0.72000\n",
            "Epoch: 410, loss: 0.71896\n",
            "Epoch: 420, loss: 0.71798\n",
            "Epoch: 430, loss: 0.71706\n",
            "Epoch: 440, loss: 0.71619\n",
            "Epoch: 450, loss: 0.71536\n",
            "Epoch: 460, loss: 0.71458\n",
            "Epoch: 470, loss: 0.71385\n",
            "Epoch: 480, loss: 0.71315\n",
            "Epoch: 490, loss: 0.71248\n",
            "Epoch: 500, loss: 0.71185\n",
            "Epoch: 510, loss: 0.71125\n",
            "Epoch: 520, loss: 0.71068\n",
            "Epoch: 530, loss: 0.71014\n",
            "Epoch: 540, loss: 0.70962\n",
            "Epoch: 550, loss: 0.70913\n",
            "Epoch: 560, loss: 0.70865\n",
            "Epoch: 570, loss: 0.70820\n",
            "Epoch: 580, loss: 0.70777\n",
            "Epoch: 590, loss: 0.70735\n",
            "Epoch: 600, loss: 0.70696\n",
            "Epoch: 610, loss: 0.70658\n",
            "Epoch: 620, loss: 0.70621\n",
            "Epoch: 630, loss: 0.70586\n",
            "Epoch: 640, loss: 0.70553\n",
            "Epoch: 650, loss: 0.70521\n",
            "Epoch: 660, loss: 0.70490\n",
            "Epoch: 670, loss: 0.70460\n",
            "Epoch: 680, loss: 0.70431\n",
            "Epoch: 690, loss: 0.70404\n",
            "Epoch: 700, loss: 0.70377\n",
            "Epoch: 710, loss: 0.70352\n",
            "Epoch: 720, loss: 0.70327\n",
            "Epoch: 730, loss: 0.70303\n",
            "Epoch: 740, loss: 0.70280\n",
            "Epoch: 750, loss: 0.70258\n",
            "Epoch: 760, loss: 0.70237\n",
            "Epoch: 770, loss: 0.70216\n",
            "Epoch: 780, loss: 0.70196\n",
            "Epoch: 790, loss: 0.70177\n",
            "Epoch: 800, loss: 0.70158\n",
            "Epoch: 810, loss: 0.70140\n",
            "Epoch: 820, loss: 0.70123\n",
            "Epoch: 830, loss: 0.70106\n",
            "Epoch: 840, loss: 0.70090\n",
            "Epoch: 850, loss: 0.70074\n",
            "Epoch: 860, loss: 0.70058\n",
            "Epoch: 870, loss: 0.70044\n",
            "Epoch: 880, loss: 0.70029\n",
            "Epoch: 890, loss: 0.70015\n",
            "Epoch: 900, loss: 0.70002\n",
            "Epoch: 910, loss: 0.69988\n",
            "Epoch: 920, loss: 0.69976\n",
            "Epoch: 930, loss: 0.69963\n",
            "Epoch: 940, loss: 0.69951\n",
            "Epoch: 950, loss: 0.69939\n",
            "Epoch: 960, loss: 0.69928\n",
            "Epoch: 970, loss: 0.69917\n",
            "Epoch: 980, loss: 0.69906\n",
            "Epoch: 990, loss: 0.69896\n",
            "Epoch: 1000, loss: 0.69886\n",
            "Epoch: 1010, loss: 0.69876\n",
            "Epoch: 1020, loss: 0.69866\n",
            "Epoch: 1030, loss: 0.69856\n",
            "Epoch: 1040, loss: 0.69847\n",
            "Epoch: 1050, loss: 0.69838\n",
            "Epoch: 1060, loss: 0.69830\n",
            "Epoch: 1070, loss: 0.69821\n",
            "Epoch: 1080, loss: 0.69813\n",
            "Epoch: 1090, loss: 0.69805\n",
            "Epoch: 1100, loss: 0.69797\n",
            "Epoch: 1110, loss: 0.69789\n",
            "Epoch: 1120, loss: 0.69782\n",
            "Epoch: 1130, loss: 0.69774\n",
            "Epoch: 1140, loss: 0.69767\n",
            "Epoch: 1150, loss: 0.69760\n",
            "Epoch: 1160, loss: 0.69754\n",
            "Epoch: 1170, loss: 0.69747\n",
            "Epoch: 1180, loss: 0.69740\n",
            "Epoch: 1190, loss: 0.69734\n",
            "Epoch: 1200, loss: 0.69728\n",
            "Epoch: 1210, loss: 0.69722\n",
            "Epoch: 1220, loss: 0.69716\n",
            "Epoch: 1230, loss: 0.69710\n",
            "Epoch: 1240, loss: 0.69704\n",
            "Epoch: 1250, loss: 0.69699\n",
            "Epoch: 1260, loss: 0.69693\n",
            "Epoch: 1270, loss: 0.69688\n",
            "Epoch: 1280, loss: 0.69683\n",
            "Epoch: 1290, loss: 0.69678\n",
            "Epoch: 1300, loss: 0.69673\n",
            "Epoch: 1310, loss: 0.69668\n",
            "Epoch: 1320, loss: 0.69663\n",
            "Epoch: 1330, loss: 0.69658\n",
            "Epoch: 1340, loss: 0.69654\n",
            "Epoch: 1350, loss: 0.69649\n",
            "Epoch: 1360, loss: 0.69645\n",
            "Epoch: 1370, loss: 0.69640\n",
            "Epoch: 1380, loss: 0.69636\n",
            "Epoch: 1390, loss: 0.69632\n",
            "Epoch: 1400, loss: 0.69628\n",
            "Epoch: 1410, loss: 0.69624\n",
            "Epoch: 1420, loss: 0.69620\n",
            "Epoch: 1430, loss: 0.69616\n",
            "Epoch: 1440, loss: 0.69612\n",
            "Epoch: 1450, loss: 0.69609\n",
            "Epoch: 1460, loss: 0.69605\n",
            "Epoch: 1470, loss: 0.69601\n",
            "Epoch: 1480, loss: 0.69598\n",
            "Epoch: 1490, loss: 0.69594\n",
            "Epoch: 1500, loss: 0.69591\n",
            "Epoch: 1510, loss: 0.69588\n",
            "Epoch: 1520, loss: 0.69584\n",
            "Epoch: 1530, loss: 0.69581\n",
            "Epoch: 1540, loss: 0.69578\n",
            "Epoch: 1550, loss: 0.69575\n",
            "Epoch: 1560, loss: 0.69572\n",
            "Epoch: 1570, loss: 0.69569\n",
            "Epoch: 1580, loss: 0.69566\n",
            "Epoch: 1590, loss: 0.69563\n",
            "Epoch: 1600, loss: 0.69560\n",
            "Epoch: 1610, loss: 0.69557\n",
            "Epoch: 1620, loss: 0.69555\n",
            "Epoch: 1630, loss: 0.69552\n",
            "Epoch: 1640, loss: 0.69549\n",
            "Epoch: 1650, loss: 0.69547\n",
            "Epoch: 1660, loss: 0.69544\n",
            "Epoch: 1670, loss: 0.69541\n",
            "Epoch: 1680, loss: 0.69539\n",
            "Epoch: 1690, loss: 0.69537\n",
            "Epoch: 1700, loss: 0.69534\n",
            "Epoch: 1710, loss: 0.69532\n",
            "Epoch: 1720, loss: 0.69529\n",
            "Epoch: 1730, loss: 0.69527\n",
            "Epoch: 1740, loss: 0.69525\n",
            "Epoch: 1750, loss: 0.69523\n",
            "Epoch: 1760, loss: 0.69520\n",
            "Epoch: 1770, loss: 0.69518\n",
            "Epoch: 1780, loss: 0.69516\n",
            "Epoch: 1790, loss: 0.69514\n",
            "Epoch: 1800, loss: 0.69512\n",
            "Epoch: 1810, loss: 0.69510\n",
            "Epoch: 1820, loss: 0.69508\n",
            "Epoch: 1830, loss: 0.69506\n",
            "Epoch: 1840, loss: 0.69504\n",
            "Epoch: 1850, loss: 0.69502\n",
            "Epoch: 1860, loss: 0.69500\n",
            "Epoch: 1870, loss: 0.69498\n",
            "Epoch: 1880, loss: 0.69496\n",
            "Epoch: 1890, loss: 0.69495\n",
            "Epoch: 1900, loss: 0.69493\n",
            "Epoch: 1910, loss: 0.69491\n",
            "Epoch: 1920, loss: 0.69489\n",
            "Epoch: 1930, loss: 0.69488\n",
            "Epoch: 1940, loss: 0.69486\n",
            "Epoch: 1950, loss: 0.69484\n",
            "Epoch: 1960, loss: 0.69483\n",
            "Epoch: 1970, loss: 0.69481\n",
            "Epoch: 1980, loss: 0.69479\n",
            "Epoch: 1990, loss: 0.69478\n",
            "Epoch: 2000, loss: 0.69476\n",
            "Epoch: 2010, loss: 0.69475\n",
            "Epoch: 2020, loss: 0.69473\n",
            "Epoch: 2030, loss: 0.69472\n",
            "Epoch: 2040, loss: 0.69470\n",
            "Epoch: 2050, loss: 0.69469\n",
            "Epoch: 2060, loss: 0.69467\n",
            "Epoch: 2070, loss: 0.69466\n",
            "Epoch: 2080, loss: 0.69464\n",
            "Epoch: 2090, loss: 0.69463\n",
            "Epoch: 2100, loss: 0.69462\n",
            "Epoch: 2110, loss: 0.69460\n",
            "Epoch: 2120, loss: 0.69459\n",
            "Epoch: 2130, loss: 0.69458\n",
            "Epoch: 2140, loss: 0.69456\n",
            "Epoch: 2150, loss: 0.69455\n",
            "Epoch: 2160, loss: 0.69454\n",
            "Epoch: 2170, loss: 0.69453\n",
            "Epoch: 2180, loss: 0.69451\n",
            "Epoch: 2190, loss: 0.69450\n",
            "Epoch: 2200, loss: 0.69449\n",
            "Epoch: 2210, loss: 0.69448\n",
            "Epoch: 2220, loss: 0.69446\n",
            "Epoch: 2230, loss: 0.69445\n",
            "Epoch: 2240, loss: 0.69444\n",
            "Epoch: 2250, loss: 0.69443\n",
            "Epoch: 2260, loss: 0.69442\n",
            "Epoch: 2270, loss: 0.69441\n",
            "Epoch: 2280, loss: 0.69440\n",
            "Epoch: 2290, loss: 0.69439\n",
            "Epoch: 2300, loss: 0.69438\n",
            "Epoch: 2310, loss: 0.69437\n",
            "Epoch: 2320, loss: 0.69436\n",
            "Epoch: 2330, loss: 0.69434\n",
            "Epoch: 2340, loss: 0.69433\n",
            "Epoch: 2350, loss: 0.69432\n",
            "Epoch: 2360, loss: 0.69431\n",
            "Epoch: 2370, loss: 0.69430\n",
            "Epoch: 2380, loss: 0.69430\n",
            "Epoch: 2390, loss: 0.69429\n",
            "Epoch: 2400, loss: 0.69428\n",
            "Epoch: 2410, loss: 0.69427\n",
            "Epoch: 2420, loss: 0.69426\n",
            "Epoch: 2430, loss: 0.69425\n",
            "Epoch: 2440, loss: 0.69424\n",
            "Epoch: 2450, loss: 0.69423\n",
            "Epoch: 2460, loss: 0.69422\n",
            "Epoch: 2470, loss: 0.69421\n",
            "Epoch: 2480, loss: 0.69420\n",
            "Epoch: 2490, loss: 0.69420\n",
            "Epoch: 2500, loss: 0.69419\n",
            "Epoch: 2510, loss: 0.69418\n",
            "Epoch: 2520, loss: 0.69417\n",
            "Epoch: 2530, loss: 0.69416\n",
            "Epoch: 2540, loss: 0.69415\n",
            "Epoch: 2550, loss: 0.69415\n",
            "Epoch: 2560, loss: 0.69414\n",
            "Epoch: 2570, loss: 0.69413\n",
            "Epoch: 2580, loss: 0.69412\n",
            "Epoch: 2590, loss: 0.69411\n",
            "Epoch: 2600, loss: 0.69411\n",
            "Epoch: 2610, loss: 0.69410\n",
            "Epoch: 2620, loss: 0.69409\n",
            "Epoch: 2630, loss: 0.69408\n",
            "Epoch: 2640, loss: 0.69408\n",
            "Epoch: 2650, loss: 0.69407\n",
            "Epoch: 2660, loss: 0.69406\n",
            "Epoch: 2670, loss: 0.69405\n",
            "Epoch: 2680, loss: 0.69405\n",
            "Epoch: 2690, loss: 0.69404\n",
            "Epoch: 2700, loss: 0.69403\n",
            "Epoch: 2710, loss: 0.69403\n",
            "Epoch: 2720, loss: 0.69402\n",
            "Epoch: 2730, loss: 0.69401\n",
            "Epoch: 2740, loss: 0.69401\n",
            "Epoch: 2750, loss: 0.69400\n",
            "Epoch: 2760, loss: 0.69399\n",
            "Epoch: 2770, loss: 0.69399\n",
            "Epoch: 2780, loss: 0.69398\n",
            "Epoch: 2790, loss: 0.69397\n",
            "Epoch: 2800, loss: 0.69397\n",
            "Epoch: 2810, loss: 0.69396\n",
            "Epoch: 2820, loss: 0.69396\n",
            "Epoch: 2830, loss: 0.69395\n",
            "Epoch: 2840, loss: 0.69394\n",
            "Epoch: 2850, loss: 0.69394\n",
            "Epoch: 2860, loss: 0.69393\n",
            "Epoch: 2870, loss: 0.69393\n",
            "Epoch: 2880, loss: 0.69392\n",
            "Epoch: 2890, loss: 0.69391\n",
            "Epoch: 2900, loss: 0.69391\n",
            "Epoch: 2910, loss: 0.69390\n",
            "Epoch: 2920, loss: 0.69390\n",
            "Epoch: 2930, loss: 0.69389\n",
            "Epoch: 2940, loss: 0.69389\n",
            "Epoch: 2950, loss: 0.69388\n",
            "Epoch: 2960, loss: 0.69388\n",
            "Epoch: 2970, loss: 0.69387\n",
            "Epoch: 2980, loss: 0.69387\n",
            "Epoch: 2990, loss: 0.69386\n",
            "Epoch: 3000, loss: 0.69386\n",
            "Epoch: 3010, loss: 0.69385\n",
            "Epoch: 3020, loss: 0.69385\n",
            "Epoch: 3030, loss: 0.69384\n",
            "Epoch: 3040, loss: 0.69384\n",
            "Epoch: 3050, loss: 0.69383\n",
            "Epoch: 3060, loss: 0.69383\n",
            "Epoch: 3070, loss: 0.69382\n",
            "Epoch: 3080, loss: 0.69382\n",
            "Epoch: 3090, loss: 0.69381\n",
            "Epoch: 3100, loss: 0.69381\n",
            "Epoch: 3110, loss: 0.69380\n",
            "Epoch: 3120, loss: 0.69380\n",
            "Epoch: 3130, loss: 0.69379\n",
            "Epoch: 3140, loss: 0.69379\n",
            "Epoch: 3150, loss: 0.69378\n",
            "Epoch: 3160, loss: 0.69378\n",
            "Epoch: 3170, loss: 0.69377\n",
            "Epoch: 3180, loss: 0.69377\n",
            "Epoch: 3190, loss: 0.69377\n",
            "Epoch: 3200, loss: 0.69376\n",
            "Epoch: 3210, loss: 0.69376\n",
            "Epoch: 3220, loss: 0.69375\n",
            "Epoch: 3230, loss: 0.69375\n",
            "Epoch: 3240, loss: 0.69375\n",
            "Epoch: 3250, loss: 0.69374\n",
            "Epoch: 3260, loss: 0.69374\n",
            "Epoch: 3270, loss: 0.69373\n",
            "Epoch: 3280, loss: 0.69373\n",
            "Epoch: 3290, loss: 0.69372\n",
            "Epoch: 3300, loss: 0.69372\n",
            "Epoch: 3310, loss: 0.69372\n",
            "Epoch: 3320, loss: 0.69371\n",
            "Epoch: 3330, loss: 0.69371\n",
            "Epoch: 3340, loss: 0.69371\n",
            "Epoch: 3350, loss: 0.69370\n",
            "Epoch: 3360, loss: 0.69370\n",
            "Epoch: 3370, loss: 0.69369\n",
            "Epoch: 3380, loss: 0.69369\n",
            "Epoch: 3390, loss: 0.69369\n",
            "Epoch: 3400, loss: 0.69368\n",
            "Epoch: 3410, loss: 0.69368\n",
            "Epoch: 3420, loss: 0.69368\n",
            "Epoch: 3430, loss: 0.69367\n",
            "Epoch: 3440, loss: 0.69367\n",
            "Epoch: 3450, loss: 0.69367\n",
            "Epoch: 3460, loss: 0.69366\n",
            "Epoch: 3470, loss: 0.69366\n",
            "Epoch: 3480, loss: 0.69366\n",
            "Epoch: 3490, loss: 0.69365\n",
            "Epoch: 3500, loss: 0.69365\n",
            "Epoch: 3510, loss: 0.69365\n",
            "Epoch: 3520, loss: 0.69364\n",
            "Epoch: 3530, loss: 0.69364\n",
            "Epoch: 3540, loss: 0.69364\n",
            "Epoch: 3550, loss: 0.69363\n",
            "Epoch: 3560, loss: 0.69363\n",
            "Epoch: 3570, loss: 0.69363\n",
            "Epoch: 3580, loss: 0.69362\n",
            "Epoch: 3590, loss: 0.69362\n",
            "Epoch: 3600, loss: 0.69362\n",
            "Epoch: 3610, loss: 0.69361\n",
            "Epoch: 3620, loss: 0.69361\n",
            "Epoch: 3630, loss: 0.69361\n",
            "Epoch: 3640, loss: 0.69360\n",
            "Epoch: 3650, loss: 0.69360\n",
            "Epoch: 3660, loss: 0.69360\n",
            "Epoch: 3670, loss: 0.69360\n",
            "Epoch: 3680, loss: 0.69359\n",
            "Epoch: 3690, loss: 0.69359\n",
            "Epoch: 3700, loss: 0.69359\n",
            "Epoch: 3710, loss: 0.69358\n",
            "Epoch: 3720, loss: 0.69358\n",
            "Epoch: 3730, loss: 0.69358\n",
            "Epoch: 3740, loss: 0.69358\n",
            "Epoch: 3750, loss: 0.69357\n",
            "Epoch: 3760, loss: 0.69357\n",
            "Epoch: 3770, loss: 0.69357\n",
            "Epoch: 3780, loss: 0.69357\n",
            "Epoch: 3790, loss: 0.69356\n",
            "Epoch: 3800, loss: 0.69356\n",
            "Epoch: 3810, loss: 0.69356\n",
            "Epoch: 3820, loss: 0.69355\n",
            "Epoch: 3830, loss: 0.69355\n",
            "Epoch: 3840, loss: 0.69355\n",
            "Epoch: 3850, loss: 0.69355\n",
            "Epoch: 3860, loss: 0.69354\n",
            "Epoch: 3870, loss: 0.69354\n",
            "Epoch: 3880, loss: 0.69354\n",
            "Epoch: 3890, loss: 0.69354\n",
            "Epoch: 3900, loss: 0.69353\n",
            "Epoch: 3910, loss: 0.69353\n",
            "Epoch: 3920, loss: 0.69353\n",
            "Epoch: 3930, loss: 0.69353\n",
            "Epoch: 3940, loss: 0.69352\n",
            "Epoch: 3950, loss: 0.69352\n",
            "Epoch: 3960, loss: 0.69352\n",
            "Epoch: 3970, loss: 0.69352\n",
            "Epoch: 3980, loss: 0.69352\n",
            "Epoch: 3990, loss: 0.69351\n",
            "Epoch: 4000, loss: 0.69351\n",
            "Epoch: 4010, loss: 0.69351\n",
            "Epoch: 4020, loss: 0.69351\n",
            "Epoch: 4030, loss: 0.69350\n",
            "Epoch: 4040, loss: 0.69350\n",
            "Epoch: 4050, loss: 0.69350\n",
            "Epoch: 4060, loss: 0.69350\n",
            "Epoch: 4070, loss: 0.69350\n",
            "Epoch: 4080, loss: 0.69349\n",
            "Epoch: 4090, loss: 0.69349\n",
            "Epoch: 4100, loss: 0.69349\n",
            "Epoch: 4110, loss: 0.69349\n",
            "Epoch: 4120, loss: 0.69349\n",
            "Epoch: 4130, loss: 0.69348\n",
            "Epoch: 4140, loss: 0.69348\n",
            "Epoch: 4150, loss: 0.69348\n",
            "Epoch: 4160, loss: 0.69348\n",
            "Epoch: 4170, loss: 0.69347\n",
            "Epoch: 4180, loss: 0.69347\n",
            "Epoch: 4190, loss: 0.69347\n",
            "Epoch: 4200, loss: 0.69347\n",
            "Epoch: 4210, loss: 0.69347\n",
            "Epoch: 4220, loss: 0.69346\n",
            "Epoch: 4230, loss: 0.69346\n",
            "Epoch: 4240, loss: 0.69346\n",
            "Epoch: 4250, loss: 0.69346\n",
            "Epoch: 4260, loss: 0.69346\n",
            "Epoch: 4270, loss: 0.69346\n",
            "Epoch: 4280, loss: 0.69345\n",
            "Epoch: 4290, loss: 0.69345\n",
            "Epoch: 4300, loss: 0.69345\n",
            "Epoch: 4310, loss: 0.69345\n",
            "Epoch: 4320, loss: 0.69345\n",
            "Epoch: 4330, loss: 0.69344\n",
            "Epoch: 4340, loss: 0.69344\n",
            "Epoch: 4350, loss: 0.69344\n",
            "Epoch: 4360, loss: 0.69344\n",
            "Epoch: 4370, loss: 0.69344\n",
            "Epoch: 4380, loss: 0.69344\n",
            "Epoch: 4390, loss: 0.69343\n",
            "Epoch: 4400, loss: 0.69343\n",
            "Epoch: 4410, loss: 0.69343\n",
            "Epoch: 4420, loss: 0.69343\n",
            "Epoch: 4430, loss: 0.69343\n",
            "Epoch: 4440, loss: 0.69343\n",
            "Epoch: 4450, loss: 0.69342\n",
            "Epoch: 4460, loss: 0.69342\n",
            "Epoch: 4470, loss: 0.69342\n",
            "Epoch: 4480, loss: 0.69342\n",
            "Epoch: 4490, loss: 0.69342\n",
            "Epoch: 4500, loss: 0.69342\n",
            "Epoch: 4510, loss: 0.69341\n",
            "Epoch: 4520, loss: 0.69341\n",
            "Epoch: 4530, loss: 0.69341\n",
            "Epoch: 4540, loss: 0.69341\n",
            "Epoch: 4550, loss: 0.69341\n",
            "Epoch: 4560, loss: 0.69341\n",
            "Epoch: 4570, loss: 0.69341\n",
            "Epoch: 4580, loss: 0.69340\n",
            "Epoch: 4590, loss: 0.69340\n",
            "Epoch: 4600, loss: 0.69340\n",
            "Epoch: 4610, loss: 0.69340\n",
            "Epoch: 4620, loss: 0.69340\n",
            "Epoch: 4630, loss: 0.69340\n",
            "Epoch: 4640, loss: 0.69339\n",
            "Epoch: 4650, loss: 0.69339\n",
            "Epoch: 4660, loss: 0.69339\n",
            "Epoch: 4670, loss: 0.69339\n",
            "Epoch: 4680, loss: 0.69339\n",
            "Epoch: 4690, loss: 0.69339\n",
            "Epoch: 4700, loss: 0.69339\n",
            "Epoch: 4710, loss: 0.69338\n",
            "Epoch: 4720, loss: 0.69338\n",
            "Epoch: 4730, loss: 0.69338\n",
            "Epoch: 4740, loss: 0.69338\n",
            "Epoch: 4750, loss: 0.69338\n",
            "Epoch: 4760, loss: 0.69338\n",
            "Epoch: 4770, loss: 0.69338\n",
            "Epoch: 4780, loss: 0.69338\n",
            "Epoch: 4790, loss: 0.69337\n",
            "Epoch: 4800, loss: 0.69337\n",
            "Epoch: 4810, loss: 0.69337\n",
            "Epoch: 4820, loss: 0.69337\n",
            "Epoch: 4830, loss: 0.69337\n",
            "Epoch: 4840, loss: 0.69337\n",
            "Epoch: 4850, loss: 0.69337\n",
            "Epoch: 4860, loss: 0.69337\n",
            "Epoch: 4870, loss: 0.69336\n",
            "Epoch: 4880, loss: 0.69336\n",
            "Epoch: 4890, loss: 0.69336\n",
            "Epoch: 4900, loss: 0.69336\n",
            "Epoch: 4910, loss: 0.69336\n",
            "Epoch: 4920, loss: 0.69336\n",
            "Epoch: 4930, loss: 0.69336\n",
            "Epoch: 4940, loss: 0.69336\n",
            "Epoch: 4950, loss: 0.69335\n",
            "Epoch: 4960, loss: 0.69335\n",
            "Epoch: 4970, loss: 0.69335\n",
            "Epoch: 4980, loss: 0.69335\n",
            "Epoch: 4990, loss: 0.69335\n",
            "Epoch: 5000, loss: 0.69335\n",
            "Epoch: 5010, loss: 0.69335\n",
            "Epoch: 5020, loss: 0.69335\n",
            "Epoch: 5030, loss: 0.69335\n",
            "Epoch: 5040, loss: 0.69334\n",
            "Epoch: 5050, loss: 0.69334\n",
            "Epoch: 5060, loss: 0.69334\n",
            "Epoch: 5070, loss: 0.69334\n",
            "Epoch: 5080, loss: 0.69334\n",
            "Epoch: 5090, loss: 0.69334\n",
            "Epoch: 5100, loss: 0.69334\n",
            "Epoch: 5110, loss: 0.69334\n",
            "Epoch: 5120, loss: 0.69334\n",
            "Epoch: 5130, loss: 0.69333\n",
            "Epoch: 5140, loss: 0.69333\n",
            "Epoch: 5150, loss: 0.69333\n",
            "Epoch: 5160, loss: 0.69333\n",
            "Epoch: 5170, loss: 0.69333\n",
            "Epoch: 5180, loss: 0.69333\n",
            "Epoch: 5190, loss: 0.69333\n",
            "Epoch: 5200, loss: 0.69333\n",
            "Epoch: 5210, loss: 0.69333\n",
            "Epoch: 5220, loss: 0.69332\n",
            "Epoch: 5230, loss: 0.69332\n",
            "Epoch: 5240, loss: 0.69332\n",
            "Epoch: 5250, loss: 0.69332\n",
            "Epoch: 5260, loss: 0.69332\n",
            "Epoch: 5270, loss: 0.69332\n",
            "Epoch: 5280, loss: 0.69332\n",
            "Epoch: 5290, loss: 0.69332\n",
            "Epoch: 5300, loss: 0.69332\n",
            "Epoch: 5310, loss: 0.69332\n",
            "Epoch: 5320, loss: 0.69332\n",
            "Epoch: 5330, loss: 0.69331\n",
            "Epoch: 5340, loss: 0.69331\n",
            "Epoch: 5350, loss: 0.69331\n",
            "Epoch: 5360, loss: 0.69331\n",
            "Epoch: 5370, loss: 0.69331\n",
            "Epoch: 5380, loss: 0.69331\n",
            "Epoch: 5390, loss: 0.69331\n",
            "Epoch: 5400, loss: 0.69331\n",
            "Epoch: 5410, loss: 0.69331\n",
            "Epoch: 5420, loss: 0.69331\n",
            "Epoch: 5430, loss: 0.69331\n",
            "Epoch: 5440, loss: 0.69330\n",
            "Epoch: 5450, loss: 0.69330\n",
            "Epoch: 5460, loss: 0.69330\n",
            "Epoch: 5470, loss: 0.69330\n",
            "Epoch: 5480, loss: 0.69330\n",
            "Epoch: 5490, loss: 0.69330\n",
            "Epoch: 5500, loss: 0.69330\n",
            "Epoch: 5510, loss: 0.69330\n",
            "Epoch: 5520, loss: 0.69330\n",
            "Epoch: 5530, loss: 0.69330\n",
            "Epoch: 5540, loss: 0.69330\n",
            "Epoch: 5550, loss: 0.69330\n",
            "Epoch: 5560, loss: 0.69329\n",
            "Epoch: 5570, loss: 0.69329\n",
            "Epoch: 5580, loss: 0.69329\n",
            "Epoch: 5590, loss: 0.69329\n",
            "Epoch: 5600, loss: 0.69329\n",
            "Epoch: 5610, loss: 0.69329\n",
            "Epoch: 5620, loss: 0.69329\n",
            "Epoch: 5630, loss: 0.69329\n",
            "Epoch: 5640, loss: 0.69329\n",
            "Epoch: 5650, loss: 0.69329\n",
            "Epoch: 5660, loss: 0.69329\n",
            "Epoch: 5670, loss: 0.69329\n",
            "Epoch: 5680, loss: 0.69328\n",
            "Epoch: 5690, loss: 0.69328\n",
            "Epoch: 5700, loss: 0.69328\n",
            "Epoch: 5710, loss: 0.69328\n",
            "Epoch: 5720, loss: 0.69328\n",
            "Epoch: 5730, loss: 0.69328\n",
            "Epoch: 5740, loss: 0.69328\n",
            "Epoch: 5750, loss: 0.69328\n",
            "Epoch: 5760, loss: 0.69328\n",
            "Epoch: 5770, loss: 0.69328\n",
            "Epoch: 5780, loss: 0.69328\n",
            "Epoch: 5790, loss: 0.69328\n",
            "Epoch: 5800, loss: 0.69328\n",
            "Epoch: 5810, loss: 0.69328\n",
            "Epoch: 5820, loss: 0.69327\n",
            "Epoch: 5830, loss: 0.69327\n",
            "Epoch: 5840, loss: 0.69327\n",
            "Epoch: 5850, loss: 0.69327\n",
            "Epoch: 5860, loss: 0.69327\n",
            "Epoch: 5870, loss: 0.69327\n",
            "Epoch: 5880, loss: 0.69327\n",
            "Epoch: 5890, loss: 0.69327\n",
            "Epoch: 5900, loss: 0.69327\n",
            "Epoch: 5910, loss: 0.69327\n",
            "Epoch: 5920, loss: 0.69327\n",
            "Epoch: 5930, loss: 0.69327\n",
            "Epoch: 5940, loss: 0.69327\n",
            "Epoch: 5950, loss: 0.69327\n",
            "Epoch: 5960, loss: 0.69327\n",
            "Epoch: 5970, loss: 0.69326\n",
            "Epoch: 5980, loss: 0.69326\n",
            "Epoch: 5990, loss: 0.69326\n",
            "Epoch: 6000, loss: 0.69326\n",
            "Epoch: 6010, loss: 0.69326\n",
            "Epoch: 6020, loss: 0.69326\n",
            "Epoch: 6030, loss: 0.69326\n",
            "Epoch: 6040, loss: 0.69326\n",
            "Epoch: 6050, loss: 0.69326\n",
            "Epoch: 6060, loss: 0.69326\n",
            "Epoch: 6070, loss: 0.69326\n",
            "Epoch: 6080, loss: 0.69326\n",
            "Epoch: 6090, loss: 0.69326\n",
            "Epoch: 6100, loss: 0.69326\n",
            "Epoch: 6110, loss: 0.69326\n",
            "Epoch: 6120, loss: 0.69326\n",
            "Epoch: 6130, loss: 0.69326\n",
            "Epoch: 6140, loss: 0.69325\n",
            "Epoch: 6150, loss: 0.69325\n",
            "Epoch: 6160, loss: 0.69325\n",
            "Epoch: 6170, loss: 0.69325\n",
            "Epoch: 6180, loss: 0.69325\n",
            "Epoch: 6190, loss: 0.69325\n",
            "Epoch: 6200, loss: 0.69325\n",
            "Epoch: 6210, loss: 0.69325\n",
            "Epoch: 6220, loss: 0.69325\n",
            "Epoch: 6230, loss: 0.69325\n",
            "Epoch: 6240, loss: 0.69325\n",
            "Epoch: 6250, loss: 0.69325\n",
            "Epoch: 6260, loss: 0.69325\n",
            "Epoch: 6270, loss: 0.69325\n",
            "Epoch: 6280, loss: 0.69325\n",
            "Epoch: 6290, loss: 0.69325\n",
            "Epoch: 6300, loss: 0.69325\n",
            "Epoch: 6310, loss: 0.69325\n",
            "Epoch: 6320, loss: 0.69324\n",
            "Epoch: 6330, loss: 0.69324\n",
            "Epoch: 6340, loss: 0.69324\n",
            "Epoch: 6350, loss: 0.69324\n",
            "Epoch: 6360, loss: 0.69324\n",
            "Epoch: 6370, loss: 0.69324\n",
            "Epoch: 6380, loss: 0.69324\n",
            "Epoch: 6390, loss: 0.69324\n",
            "Epoch: 6400, loss: 0.69324\n",
            "Epoch: 6410, loss: 0.69324\n",
            "Epoch: 6420, loss: 0.69324\n",
            "Epoch: 6430, loss: 0.69324\n",
            "Epoch: 6440, loss: 0.69324\n",
            "Epoch: 6450, loss: 0.69324\n",
            "Epoch: 6460, loss: 0.69324\n",
            "Epoch: 6470, loss: 0.69324\n",
            "Epoch: 6480, loss: 0.69324\n",
            "Epoch: 6490, loss: 0.69324\n",
            "Epoch: 6500, loss: 0.69324\n",
            "Epoch: 6510, loss: 0.69324\n",
            "Epoch: 6520, loss: 0.69323\n",
            "Epoch: 6530, loss: 0.69323\n",
            "Epoch: 6540, loss: 0.69323\n",
            "Epoch: 6550, loss: 0.69323\n",
            "Epoch: 6560, loss: 0.69323\n",
            "Epoch: 6570, loss: 0.69323\n",
            "Epoch: 6580, loss: 0.69323\n",
            "Epoch: 6590, loss: 0.69323\n",
            "Epoch: 6600, loss: 0.69323\n",
            "Epoch: 6610, loss: 0.69323\n",
            "Epoch: 6620, loss: 0.69323\n",
            "Epoch: 6630, loss: 0.69323\n",
            "Epoch: 6640, loss: 0.69323\n",
            "Epoch: 6650, loss: 0.69323\n",
            "Epoch: 6660, loss: 0.69323\n",
            "Epoch: 6670, loss: 0.69323\n",
            "Epoch: 6680, loss: 0.69323\n",
            "Epoch: 6690, loss: 0.69323\n",
            "Epoch: 6700, loss: 0.69323\n",
            "Epoch: 6710, loss: 0.69323\n",
            "Epoch: 6720, loss: 0.69323\n",
            "Epoch: 6730, loss: 0.69323\n",
            "Epoch: 6740, loss: 0.69323\n",
            "Epoch: 6750, loss: 0.69322\n",
            "Epoch: 6760, loss: 0.69322\n",
            "Epoch: 6770, loss: 0.69322\n",
            "Epoch: 6780, loss: 0.69322\n",
            "Epoch: 6790, loss: 0.69322\n",
            "Epoch: 6800, loss: 0.69322\n",
            "Epoch: 6810, loss: 0.69322\n",
            "Epoch: 6820, loss: 0.69322\n",
            "Epoch: 6830, loss: 0.69322\n",
            "Epoch: 6840, loss: 0.69322\n",
            "Epoch: 6850, loss: 0.69322\n",
            "Epoch: 6860, loss: 0.69322\n",
            "Epoch: 6870, loss: 0.69322\n",
            "Epoch: 6880, loss: 0.69322\n",
            "Epoch: 6890, loss: 0.69322\n",
            "Epoch: 6900, loss: 0.69322\n",
            "Epoch: 6910, loss: 0.69322\n",
            "Epoch: 6920, loss: 0.69322\n",
            "Epoch: 6930, loss: 0.69322\n",
            "Epoch: 6940, loss: 0.69322\n",
            "Epoch: 6950, loss: 0.69322\n",
            "Epoch: 6960, loss: 0.69322\n",
            "Epoch: 6970, loss: 0.69322\n",
            "Epoch: 6980, loss: 0.69322\n",
            "Epoch: 6990, loss: 0.69322\n",
            "Epoch: 7000, loss: 0.69322\n",
            "Epoch: 7010, loss: 0.69321\n",
            "Epoch: 7020, loss: 0.69321\n",
            "Epoch: 7030, loss: 0.69321\n",
            "Epoch: 7040, loss: 0.69321\n",
            "Epoch: 7050, loss: 0.69321\n",
            "Epoch: 7060, loss: 0.69321\n",
            "Epoch: 7070, loss: 0.69321\n",
            "Epoch: 7080, loss: 0.69321\n",
            "Epoch: 7090, loss: 0.69321\n",
            "Epoch: 7100, loss: 0.69321\n",
            "Epoch: 7110, loss: 0.69321\n",
            "Epoch: 7120, loss: 0.69321\n",
            "Epoch: 7130, loss: 0.69321\n",
            "Epoch: 7140, loss: 0.69321\n",
            "Epoch: 7150, loss: 0.69321\n",
            "Epoch: 7160, loss: 0.69321\n",
            "Epoch: 7170, loss: 0.69321\n",
            "Epoch: 7180, loss: 0.69321\n",
            "Epoch: 7190, loss: 0.69321\n",
            "Epoch: 7200, loss: 0.69321\n",
            "Epoch: 7210, loss: 0.69321\n",
            "Epoch: 7220, loss: 0.69321\n",
            "Epoch: 7230, loss: 0.69321\n",
            "Epoch: 7240, loss: 0.69321\n",
            "Epoch: 7250, loss: 0.69321\n",
            "Epoch: 7260, loss: 0.69321\n",
            "Epoch: 7270, loss: 0.69321\n",
            "Epoch: 7280, loss: 0.69321\n",
            "Epoch: 7290, loss: 0.69321\n",
            "Epoch: 7300, loss: 0.69321\n",
            "Epoch: 7310, loss: 0.69321\n",
            "Epoch: 7320, loss: 0.69320\n",
            "Epoch: 7330, loss: 0.69320\n",
            "Epoch: 7340, loss: 0.69320\n",
            "Epoch: 7350, loss: 0.69320\n",
            "Epoch: 7360, loss: 0.69320\n",
            "Epoch: 7370, loss: 0.69320\n",
            "Epoch: 7380, loss: 0.69320\n",
            "Epoch: 7390, loss: 0.69320\n",
            "Epoch: 7400, loss: 0.69320\n",
            "Epoch: 7410, loss: 0.69320\n",
            "Epoch: 7420, loss: 0.69320\n",
            "Epoch: 7430, loss: 0.69320\n",
            "Epoch: 7440, loss: 0.69320\n",
            "Epoch: 7450, loss: 0.69320\n",
            "Epoch: 7460, loss: 0.69320\n",
            "Epoch: 7470, loss: 0.69320\n",
            "Epoch: 7480, loss: 0.69320\n",
            "Epoch: 7490, loss: 0.69320\n",
            "Epoch: 7500, loss: 0.69320\n",
            "Epoch: 7510, loss: 0.69320\n",
            "Epoch: 7520, loss: 0.69320\n",
            "Epoch: 7530, loss: 0.69320\n",
            "Epoch: 7540, loss: 0.69320\n",
            "Epoch: 7550, loss: 0.69320\n",
            "Epoch: 7560, loss: 0.69320\n",
            "Epoch: 7570, loss: 0.69320\n",
            "Epoch: 7580, loss: 0.69320\n",
            "Epoch: 7590, loss: 0.69320\n",
            "Epoch: 7600, loss: 0.69320\n",
            "Epoch: 7610, loss: 0.69320\n",
            "Epoch: 7620, loss: 0.69320\n",
            "Epoch: 7630, loss: 0.69320\n",
            "Epoch: 7640, loss: 0.69320\n",
            "Epoch: 7650, loss: 0.69320\n",
            "Epoch: 7660, loss: 0.69320\n",
            "Epoch: 7670, loss: 0.69320\n",
            "Epoch: 7680, loss: 0.69319\n",
            "Epoch: 7690, loss: 0.69319\n",
            "Epoch: 7700, loss: 0.69319\n",
            "Epoch: 7710, loss: 0.69319\n",
            "Epoch: 7720, loss: 0.69319\n",
            "Epoch: 7730, loss: 0.69319\n",
            "Epoch: 7740, loss: 0.69319\n",
            "Epoch: 7750, loss: 0.69319\n",
            "Epoch: 7760, loss: 0.69319\n",
            "Epoch: 7770, loss: 0.69319\n",
            "Epoch: 7780, loss: 0.69319\n",
            "Epoch: 7790, loss: 0.69319\n",
            "Epoch: 7800, loss: 0.69319\n",
            "Epoch: 7810, loss: 0.69319\n",
            "Epoch: 7820, loss: 0.69319\n",
            "Epoch: 7830, loss: 0.69319\n",
            "Epoch: 7840, loss: 0.69319\n",
            "Epoch: 7850, loss: 0.69319\n",
            "Epoch: 7860, loss: 0.69319\n",
            "Epoch: 7870, loss: 0.69319\n",
            "Epoch: 7880, loss: 0.69319\n",
            "Epoch: 7890, loss: 0.69319\n",
            "Epoch: 7900, loss: 0.69319\n",
            "Epoch: 7910, loss: 0.69319\n",
            "Epoch: 7920, loss: 0.69319\n",
            "Epoch: 7930, loss: 0.69319\n",
            "Epoch: 7940, loss: 0.69319\n",
            "Epoch: 7950, loss: 0.69319\n",
            "Epoch: 7960, loss: 0.69319\n",
            "Epoch: 7970, loss: 0.69319\n",
            "Epoch: 7980, loss: 0.69319\n",
            "Epoch: 7990, loss: 0.69319\n",
            "Epoch: 8000, loss: 0.69319\n",
            "Epoch: 8010, loss: 0.69319\n",
            "Epoch: 8020, loss: 0.69319\n",
            "Epoch: 8030, loss: 0.69319\n",
            "Epoch: 8040, loss: 0.69319\n",
            "Epoch: 8050, loss: 0.69319\n",
            "Epoch: 8060, loss: 0.69319\n",
            "Epoch: 8070, loss: 0.69319\n",
            "Epoch: 8080, loss: 0.69319\n",
            "Epoch: 8090, loss: 0.69319\n",
            "Epoch: 8100, loss: 0.69319\n",
            "Epoch: 8110, loss: 0.69319\n",
            "Epoch: 8120, loss: 0.69319\n",
            "Epoch: 8130, loss: 0.69319\n",
            "Epoch: 8140, loss: 0.69318\n",
            "Epoch: 8150, loss: 0.69318\n",
            "Epoch: 8160, loss: 0.69318\n",
            "Epoch: 8170, loss: 0.69318\n",
            "Epoch: 8180, loss: 0.69318\n",
            "Epoch: 8190, loss: 0.69318\n",
            "Epoch: 8200, loss: 0.69318\n",
            "Epoch: 8210, loss: 0.69318\n",
            "Epoch: 8220, loss: 0.69318\n",
            "Epoch: 8230, loss: 0.69318\n",
            "Epoch: 8240, loss: 0.69318\n",
            "Epoch: 8250, loss: 0.69318\n",
            "Epoch: 8260, loss: 0.69318\n",
            "Epoch: 8270, loss: 0.69318\n",
            "Epoch: 8280, loss: 0.69318\n",
            "Epoch: 8290, loss: 0.69318\n",
            "Epoch: 8300, loss: 0.69318\n",
            "Epoch: 8310, loss: 0.69318\n",
            "Epoch: 8320, loss: 0.69318\n",
            "Epoch: 8330, loss: 0.69318\n",
            "Epoch: 8340, loss: 0.69318\n",
            "Epoch: 8350, loss: 0.69318\n",
            "Epoch: 8360, loss: 0.69318\n",
            "Epoch: 8370, loss: 0.69318\n",
            "Epoch: 8380, loss: 0.69318\n",
            "Epoch: 8390, loss: 0.69318\n",
            "Epoch: 8400, loss: 0.69318\n",
            "Epoch: 8410, loss: 0.69318\n",
            "Epoch: 8420, loss: 0.69318\n",
            "Epoch: 8430, loss: 0.69318\n",
            "Epoch: 8440, loss: 0.69318\n",
            "Epoch: 8450, loss: 0.69318\n",
            "Epoch: 8460, loss: 0.69318\n",
            "Epoch: 8470, loss: 0.69318\n",
            "Epoch: 8480, loss: 0.69318\n",
            "Epoch: 8490, loss: 0.69318\n",
            "Epoch: 8500, loss: 0.69318\n",
            "Epoch: 8510, loss: 0.69318\n",
            "Epoch: 8520, loss: 0.69318\n",
            "Epoch: 8530, loss: 0.69318\n",
            "Epoch: 8540, loss: 0.69318\n",
            "Epoch: 8550, loss: 0.69318\n",
            "Epoch: 8560, loss: 0.69318\n",
            "Epoch: 8570, loss: 0.69318\n",
            "Epoch: 8580, loss: 0.69318\n",
            "Epoch: 8590, loss: 0.69318\n",
            "Epoch: 8600, loss: 0.69318\n",
            "Epoch: 8610, loss: 0.69318\n",
            "Epoch: 8620, loss: 0.69318\n",
            "Epoch: 8630, loss: 0.69318\n",
            "Epoch: 8640, loss: 0.69318\n",
            "Epoch: 8650, loss: 0.69318\n",
            "Epoch: 8660, loss: 0.69318\n",
            "Epoch: 8670, loss: 0.69318\n",
            "Epoch: 8680, loss: 0.69318\n",
            "Epoch: 8690, loss: 0.69318\n",
            "Epoch: 8700, loss: 0.69318\n",
            "Epoch: 8710, loss: 0.69318\n",
            "Epoch: 8720, loss: 0.69318\n",
            "Epoch: 8730, loss: 0.69318\n",
            "Epoch: 8740, loss: 0.69317\n",
            "Epoch: 8750, loss: 0.69317\n",
            "Epoch: 8760, loss: 0.69317\n",
            "Epoch: 8770, loss: 0.69317\n",
            "Epoch: 8780, loss: 0.69317\n",
            "Epoch: 8790, loss: 0.69317\n",
            "Epoch: 8800, loss: 0.69317\n",
            "Epoch: 8810, loss: 0.69317\n",
            "Epoch: 8820, loss: 0.69317\n",
            "Epoch: 8830, loss: 0.69317\n",
            "Epoch: 8840, loss: 0.69317\n",
            "Epoch: 8850, loss: 0.69317\n",
            "Epoch: 8860, loss: 0.69317\n",
            "Epoch: 8870, loss: 0.69317\n",
            "Epoch: 8880, loss: 0.69317\n",
            "Epoch: 8890, loss: 0.69317\n",
            "Epoch: 8900, loss: 0.69317\n",
            "Epoch: 8910, loss: 0.69317\n",
            "Epoch: 8920, loss: 0.69317\n",
            "Epoch: 8930, loss: 0.69317\n",
            "Epoch: 8940, loss: 0.69317\n",
            "Epoch: 8950, loss: 0.69317\n",
            "Epoch: 8960, loss: 0.69317\n",
            "Epoch: 8970, loss: 0.69317\n",
            "Epoch: 8980, loss: 0.69317\n",
            "Epoch: 8990, loss: 0.69317\n",
            "Epoch: 9000, loss: 0.69317\n",
            "Epoch: 9010, loss: 0.69317\n",
            "Epoch: 9020, loss: 0.69317\n",
            "Epoch: 9030, loss: 0.69317\n",
            "Epoch: 9040, loss: 0.69317\n",
            "Epoch: 9050, loss: 0.69317\n",
            "Epoch: 9060, loss: 0.69317\n",
            "Epoch: 9070, loss: 0.69317\n",
            "Epoch: 9080, loss: 0.69317\n",
            "Epoch: 9090, loss: 0.69317\n",
            "Epoch: 9100, loss: 0.69317\n",
            "Epoch: 9110, loss: 0.69317\n",
            "Epoch: 9120, loss: 0.69317\n",
            "Epoch: 9130, loss: 0.69317\n",
            "Epoch: 9140, loss: 0.69317\n",
            "Epoch: 9150, loss: 0.69317\n",
            "Epoch: 9160, loss: 0.69317\n",
            "Epoch: 9170, loss: 0.69317\n",
            "Epoch: 9180, loss: 0.69317\n",
            "Epoch: 9190, loss: 0.69317\n",
            "Epoch: 9200, loss: 0.69317\n",
            "Epoch: 9210, loss: 0.69317\n",
            "Epoch: 9220, loss: 0.69317\n",
            "Epoch: 9230, loss: 0.69317\n",
            "Epoch: 9240, loss: 0.69317\n",
            "Epoch: 9250, loss: 0.69317\n",
            "Epoch: 9260, loss: 0.69317\n",
            "Epoch: 9270, loss: 0.69317\n",
            "Epoch: 9280, loss: 0.69317\n",
            "Epoch: 9290, loss: 0.69317\n",
            "Epoch: 9300, loss: 0.69317\n",
            "Epoch: 9310, loss: 0.69317\n",
            "Epoch: 9320, loss: 0.69317\n",
            "Epoch: 9330, loss: 0.69317\n",
            "Epoch: 9340, loss: 0.69317\n",
            "Epoch: 9350, loss: 0.69317\n",
            "Epoch: 9360, loss: 0.69317\n",
            "Epoch: 9370, loss: 0.69317\n",
            "Epoch: 9380, loss: 0.69317\n",
            "Epoch: 9390, loss: 0.69317\n",
            "Epoch: 9400, loss: 0.69317\n",
            "Epoch: 9410, loss: 0.69317\n",
            "Epoch: 9420, loss: 0.69317\n",
            "Epoch: 9430, loss: 0.69317\n",
            "Epoch: 9440, loss: 0.69317\n",
            "Epoch: 9450, loss: 0.69317\n",
            "Epoch: 9460, loss: 0.69317\n",
            "Epoch: 9470, loss: 0.69317\n",
            "Epoch: 9480, loss: 0.69317\n",
            "Epoch: 9490, loss: 0.69317\n",
            "Epoch: 9500, loss: 0.69317\n",
            "Epoch: 9510, loss: 0.69317\n",
            "Epoch: 9520, loss: 0.69317\n",
            "Epoch: 9530, loss: 0.69317\n",
            "Epoch: 9540, loss: 0.69317\n",
            "Epoch: 9550, loss: 0.69317\n",
            "Epoch: 9560, loss: 0.69317\n",
            "Epoch: 9570, loss: 0.69317\n",
            "Epoch: 9580, loss: 0.69317\n",
            "Epoch: 9590, loss: 0.69317\n",
            "Epoch: 9600, loss: 0.69316\n",
            "Epoch: 9610, loss: 0.69316\n",
            "Epoch: 9620, loss: 0.69316\n",
            "Epoch: 9630, loss: 0.69316\n",
            "Epoch: 9640, loss: 0.69316\n",
            "Epoch: 9650, loss: 0.69316\n",
            "Epoch: 9660, loss: 0.69316\n",
            "Epoch: 9670, loss: 0.69316\n",
            "Epoch: 9680, loss: 0.69316\n",
            "Epoch: 9690, loss: 0.69316\n",
            "Epoch: 9700, loss: 0.69316\n",
            "Epoch: 9710, loss: 0.69316\n",
            "Epoch: 9720, loss: 0.69316\n",
            "Epoch: 9730, loss: 0.69316\n",
            "Epoch: 9740, loss: 0.69316\n",
            "Epoch: 9750, loss: 0.69316\n",
            "Epoch: 9760, loss: 0.69316\n",
            "Epoch: 9770, loss: 0.69316\n",
            "Epoch: 9780, loss: 0.69316\n",
            "Epoch: 9790, loss: 0.69316\n",
            "Epoch: 9800, loss: 0.69316\n",
            "Epoch: 9810, loss: 0.69316\n",
            "Epoch: 9820, loss: 0.69316\n",
            "Epoch: 9830, loss: 0.69316\n",
            "Epoch: 9840, loss: 0.69316\n",
            "Epoch: 9850, loss: 0.69316\n",
            "Epoch: 9860, loss: 0.69316\n",
            "Epoch: 9870, loss: 0.69316\n",
            "Epoch: 9880, loss: 0.69316\n",
            "Epoch: 9890, loss: 0.69316\n",
            "Epoch: 9900, loss: 0.69316\n",
            "Epoch: 9910, loss: 0.69316\n",
            "Epoch: 9920, loss: 0.69316\n",
            "Epoch: 9930, loss: 0.69316\n",
            "Epoch: 9940, loss: 0.69316\n",
            "Epoch: 9950, loss: 0.69316\n",
            "Epoch: 9960, loss: 0.69316\n",
            "Epoch: 9970, loss: 0.69316\n",
            "Epoch: 9980, loss: 0.69316\n",
            "Epoch: 9990, loss: 0.69316\n",
            "Epoch: 10000, loss: 0.69316\n",
            "Epoch: 10010, loss: 0.69316\n",
            "Epoch: 10020, loss: 0.69316\n",
            "Epoch: 10030, loss: 0.69316\n",
            "Epoch: 10040, loss: 0.69316\n",
            "Epoch: 10050, loss: 0.69316\n",
            "Epoch: 10060, loss: 0.69316\n",
            "Epoch: 10070, loss: 0.69316\n",
            "Epoch: 10080, loss: 0.69316\n",
            "Epoch: 10090, loss: 0.69316\n",
            "Epoch: 10100, loss: 0.69316\n",
            "Epoch: 10110, loss: 0.69316\n",
            "Epoch: 10120, loss: 0.69316\n",
            "Epoch: 10130, loss: 0.69316\n",
            "Epoch: 10140, loss: 0.69316\n",
            "Epoch: 10150, loss: 0.69316\n",
            "Epoch: 10160, loss: 0.69316\n",
            "Epoch: 10170, loss: 0.69316\n",
            "Epoch: 10180, loss: 0.69316\n",
            "Epoch: 10190, loss: 0.69316\n",
            "Epoch: 10200, loss: 0.69316\n",
            "Epoch: 10210, loss: 0.69316\n",
            "Epoch: 10220, loss: 0.69316\n",
            "Epoch: 10230, loss: 0.69316\n",
            "Epoch: 10240, loss: 0.69316\n",
            "Epoch: 10250, loss: 0.69316\n",
            "Epoch: 10260, loss: 0.69316\n",
            "Epoch: 10270, loss: 0.69316\n",
            "Epoch: 10280, loss: 0.69316\n",
            "Epoch: 10290, loss: 0.69316\n",
            "Epoch: 10300, loss: 0.69316\n",
            "Epoch: 10310, loss: 0.69316\n",
            "Epoch: 10320, loss: 0.69316\n",
            "Epoch: 10330, loss: 0.69316\n",
            "Epoch: 10340, loss: 0.69316\n",
            "Epoch: 10350, loss: 0.69316\n",
            "Epoch: 10360, loss: 0.69316\n",
            "Epoch: 10370, loss: 0.69316\n",
            "Epoch: 10380, loss: 0.69316\n",
            "Epoch: 10390, loss: 0.69316\n",
            "Epoch: 10400, loss: 0.69316\n",
            "Epoch: 10410, loss: 0.69316\n",
            "Epoch: 10420, loss: 0.69316\n",
            "Epoch: 10430, loss: 0.69316\n",
            "Epoch: 10440, loss: 0.69316\n",
            "Epoch: 10450, loss: 0.69316\n",
            "Epoch: 10460, loss: 0.69316\n",
            "Epoch: 10470, loss: 0.69316\n",
            "Epoch: 10480, loss: 0.69316\n",
            "Epoch: 10490, loss: 0.69316\n",
            "Epoch: 10500, loss: 0.69316\n",
            "Epoch: 10510, loss: 0.69316\n",
            "Epoch: 10520, loss: 0.69316\n",
            "Epoch: 10530, loss: 0.69316\n",
            "Epoch: 10540, loss: 0.69316\n",
            "Epoch: 10550, loss: 0.69316\n",
            "Epoch: 10560, loss: 0.69316\n",
            "Epoch: 10570, loss: 0.69316\n",
            "Epoch: 10580, loss: 0.69316\n",
            "Epoch: 10590, loss: 0.69316\n",
            "Epoch: 10600, loss: 0.69316\n",
            "Epoch: 10610, loss: 0.69316\n",
            "Epoch: 10620, loss: 0.69316\n",
            "Epoch: 10630, loss: 0.69316\n",
            "Epoch: 10640, loss: 0.69316\n",
            "Epoch: 10650, loss: 0.69316\n",
            "Epoch: 10660, loss: 0.69316\n",
            "Epoch: 10670, loss: 0.69316\n",
            "Epoch: 10680, loss: 0.69316\n",
            "Epoch: 10690, loss: 0.69316\n",
            "Epoch: 10700, loss: 0.69316\n",
            "Epoch: 10710, loss: 0.69316\n",
            "Epoch: 10720, loss: 0.69316\n",
            "Epoch: 10730, loss: 0.69316\n",
            "Epoch: 10740, loss: 0.69316\n",
            "Epoch: 10750, loss: 0.69316\n",
            "Epoch: 10760, loss: 0.69316\n",
            "Epoch: 10770, loss: 0.69316\n",
            "Epoch: 10780, loss: 0.69316\n",
            "Epoch: 10790, loss: 0.69316\n",
            "Epoch: 10800, loss: 0.69316\n",
            "Epoch: 10810, loss: 0.69316\n",
            "Epoch: 10820, loss: 0.69316\n",
            "Epoch: 10830, loss: 0.69316\n",
            "Epoch: 10840, loss: 0.69316\n",
            "Epoch: 10850, loss: 0.69316\n",
            "Epoch: 10860, loss: 0.69316\n",
            "Epoch: 10870, loss: 0.69316\n",
            "Epoch: 10880, loss: 0.69316\n",
            "Epoch: 10890, loss: 0.69316\n",
            "Epoch: 10900, loss: 0.69316\n",
            "Epoch: 10910, loss: 0.69316\n",
            "Epoch: 10920, loss: 0.69316\n",
            "Epoch: 10930, loss: 0.69316\n",
            "Epoch: 10940, loss: 0.69316\n",
            "Epoch: 10950, loss: 0.69316\n",
            "Epoch: 10960, loss: 0.69316\n",
            "Epoch: 10970, loss: 0.69316\n",
            "Epoch: 10980, loss: 0.69316\n",
            "Epoch: 10990, loss: 0.69316\n",
            "Epoch: 11000, loss: 0.69316\n",
            "Epoch: 11010, loss: 0.69316\n",
            "Epoch: 11020, loss: 0.69316\n",
            "Epoch: 11030, loss: 0.69316\n",
            "Epoch: 11040, loss: 0.69316\n",
            "Epoch: 11050, loss: 0.69316\n",
            "Epoch: 11060, loss: 0.69316\n",
            "Epoch: 11070, loss: 0.69316\n",
            "Epoch: 11080, loss: 0.69316\n",
            "Epoch: 11090, loss: 0.69316\n",
            "Epoch: 11100, loss: 0.69316\n",
            "Epoch: 11110, loss: 0.69316\n",
            "Epoch: 11120, loss: 0.69316\n",
            "Epoch: 11130, loss: 0.69316\n",
            "Epoch: 11140, loss: 0.69316\n",
            "Epoch: 11150, loss: 0.69316\n",
            "Epoch: 11160, loss: 0.69316\n",
            "Epoch: 11170, loss: 0.69315\n",
            "Epoch: 11180, loss: 0.69316\n",
            "Epoch: 11190, loss: 0.69315\n",
            "Epoch: 11200, loss: 0.69315\n",
            "Epoch: 11210, loss: 0.69315\n",
            "Epoch: 11220, loss: 0.69315\n",
            "Epoch: 11230, loss: 0.69315\n",
            "Epoch: 11240, loss: 0.69315\n",
            "Epoch: 11250, loss: 0.69315\n",
            "Epoch: 11260, loss: 0.69315\n",
            "Epoch: 11270, loss: 0.69315\n",
            "Epoch: 11280, loss: 0.69315\n",
            "Epoch: 11290, loss: 0.69315\n",
            "Epoch: 11300, loss: 0.69315\n",
            "Epoch: 11310, loss: 0.69315\n",
            "Epoch: 11320, loss: 0.69315\n",
            "Epoch: 11330, loss: 0.69315\n",
            "Epoch: 11340, loss: 0.69315\n",
            "Epoch: 11350, loss: 0.69315\n",
            "Epoch: 11360, loss: 0.69315\n",
            "Epoch: 11370, loss: 0.69315\n",
            "Epoch: 11380, loss: 0.69315\n",
            "Epoch: 11390, loss: 0.69315\n",
            "Epoch: 11400, loss: 0.69315\n",
            "Epoch: 11410, loss: 0.69315\n",
            "Epoch: 11420, loss: 0.69315\n",
            "Epoch: 11430, loss: 0.69314\n",
            "Epoch: 11440, loss: 0.69313\n",
            "Epoch: 11450, loss: 0.69311\n",
            "Epoch: 11460, loss: 0.69310\n",
            "Epoch: 11470, loss: 0.69308\n",
            "Epoch: 11480, loss: 0.69306\n",
            "Epoch: 11490, loss: 0.69303\n",
            "Epoch: 11500, loss: 0.69295\n",
            "Epoch: 11510, loss: 0.69579\n",
            "Epoch: 11520, loss: 0.69296\n",
            "Epoch: 11530, loss: 0.69295\n",
            "Epoch: 11540, loss: 0.69292\n",
            "Epoch: 11550, loss: 0.69290\n",
            "Epoch: 11560, loss: 0.69286\n",
            "Epoch: 11570, loss: 0.69282\n",
            "Epoch: 11580, loss: 0.69277\n",
            "Epoch: 11590, loss: 0.69272\n",
            "Epoch: 11600, loss: 0.69266\n",
            "Epoch: 11610, loss: 0.69259\n",
            "Epoch: 11620, loss: 0.69252\n",
            "Epoch: 11630, loss: 0.69243\n",
            "Epoch: 11640, loss: 0.69231\n",
            "Epoch: 11650, loss: 0.69213\n",
            "Epoch: 11660, loss: 0.69153\n",
            "Epoch: 11670, loss: 0.68985\n",
            "Epoch: 11680, loss: 0.68690\n",
            "Epoch: 11690, loss: 0.67627\n",
            "Epoch: 11700, loss: 0.58000\n",
            "Epoch: 11710, loss: 0.39555\n",
            "Epoch: 11720, loss: 0.18673\n",
            "Epoch: 11730, loss: 0.08768\n",
            "Epoch: 11740, loss: 0.04281\n",
            "Epoch: 11750, loss: 0.02886\n",
            "Epoch: 11760, loss: 0.02562\n",
            "Epoch: 11770, loss: 0.01798\n",
            "Epoch: 11780, loss: 0.01360\n",
            "Epoch: 11790, loss: 0.01663\n",
            "Epoch: 11800, loss: 0.01727\n",
            "Epoch: 11810, loss: 0.00947\n",
            "Epoch: 11820, loss: 0.00865\n",
            "Epoch: 11830, loss: 0.00781\n",
            "Epoch: 11840, loss: 0.00714\n",
            "Epoch: 11850, loss: 0.00657\n",
            "Epoch: 11860, loss: 0.00785\n",
            "Epoch: 11870, loss: 0.00574\n",
            "Epoch: 11880, loss: 0.00542\n",
            "Epoch: 11890, loss: 0.00511\n",
            "Epoch: 11900, loss: 0.00741\n",
            "Epoch: 11910, loss: 0.00447\n",
            "Epoch: 11920, loss: 0.00423\n",
            "Epoch: 11930, loss: 0.00634\n",
            "Epoch: 11940, loss: 0.00399\n",
            "Epoch: 11950, loss: 0.00393\n",
            "Epoch: 11960, loss: 0.00377\n",
            "Epoch: 11970, loss: 0.00623\n",
            "Epoch: 11980, loss: 0.00622\n",
            "Epoch: 11990, loss: 0.00315\n",
            "Epoch: 12000, loss: 0.00298\n",
            "Epoch: 12010, loss: 0.00289\n",
            "Epoch: 12020, loss: 0.00283\n",
            "Epoch: 12030, loss: 0.00272\n",
            "Epoch: 12040, loss: 0.00260\n",
            "Epoch: 12050, loss: 0.00448\n",
            "Epoch: 12060, loss: 0.00243\n",
            "Epoch: 12070, loss: 0.00240\n",
            "Epoch: 12080, loss: 0.00232\n",
            "Epoch: 12090, loss: 0.00220\n",
            "Epoch: 12100, loss: 0.00209\n",
            "Epoch: 12110, loss: 0.00200\n",
            "Epoch: 12120, loss: 0.00192\n",
            "Epoch: 12130, loss: 0.00184\n",
            "Epoch: 12140, loss: 0.00179\n",
            "Epoch: 12150, loss: 0.00174\n",
            "Epoch: 12160, loss: 0.00289\n",
            "Epoch: 12170, loss: 0.00166\n",
            "Epoch: 12180, loss: 0.00163\n",
            "Epoch: 12190, loss: 0.00158\n",
            "Epoch: 12200, loss: 0.00153\n",
            "Epoch: 12210, loss: 0.00147\n",
            "Epoch: 12220, loss: 0.00142\n",
            "Epoch: 12230, loss: 0.00138\n",
            "Epoch: 12240, loss: 0.00140\n",
            "Epoch: 12250, loss: 0.00137\n",
            "Epoch: 12260, loss: 0.00133\n",
            "Epoch: 12270, loss: 0.00128\n",
            "Epoch: 12280, loss: 0.00124\n",
            "Epoch: 12290, loss: 0.00118\n",
            "Epoch: 12300, loss: 0.00114\n",
            "Epoch: 12310, loss: 0.00110\n",
            "Epoch: 12320, loss: 0.00107\n",
            "Epoch: 12330, loss: 0.00104\n",
            "Epoch: 12340, loss: 0.00101\n",
            "Epoch: 12350, loss: 0.00098\n",
            "Epoch: 12360, loss: 0.00440\n",
            "Epoch: 12370, loss: 0.00441\n",
            "Epoch: 12380, loss: 0.00093\n",
            "Epoch: 12390, loss: 0.00431\n",
            "Epoch: 12400, loss: 0.00089\n",
            "Epoch: 12410, loss: 0.00087\n",
            "Epoch: 12420, loss: 0.00085\n",
            "Epoch: 12430, loss: 0.00083\n",
            "Epoch: 12440, loss: 0.00434\n",
            "Epoch: 12450, loss: 0.00079\n",
            "Epoch: 12460, loss: 0.00077\n",
            "Epoch: 12470, loss: 0.00075\n",
            "Epoch: 12480, loss: 0.00074\n",
            "Epoch: 12490, loss: 0.00072\n",
            "Epoch: 12500, loss: 0.00071\n",
            "Epoch: 12510, loss: 0.00069\n",
            "Epoch: 12520, loss: 0.00068\n",
            "Epoch: 12530, loss: 0.00066\n",
            "Epoch: 12540, loss: 0.00065\n",
            "Epoch: 12550, loss: 0.00064\n",
            "Epoch: 12560, loss: 0.00063\n",
            "Epoch: 12570, loss: 0.00062\n",
            "Epoch: 12580, loss: 0.00061\n",
            "Epoch: 12590, loss: 0.00060\n",
            "Epoch: 12600, loss: 0.00059\n",
            "Epoch: 12610, loss: 0.00058\n",
            "Epoch: 12620, loss: 0.00057\n",
            "Epoch: 12630, loss: 0.00057\n",
            "Epoch: 12640, loss: 0.00056\n",
            "Epoch: 12650, loss: 0.00055\n",
            "Epoch: 12660, loss: 0.00055\n",
            "Epoch: 12670, loss: 0.00054\n",
            "Epoch: 12680, loss: 0.00053\n",
            "Epoch: 12690, loss: 0.00052\n",
            "Epoch: 12700, loss: 0.00055\n",
            "Epoch: 12710, loss: 0.00055\n",
            "Epoch: 12720, loss: 0.00053\n",
            "Epoch: 12730, loss: 0.00051\n",
            "Epoch: 12740, loss: 0.00050\n",
            "Epoch: 12750, loss: 0.00049\n",
            "Epoch: 12760, loss: 0.00048\n",
            "Epoch: 12770, loss: 0.00047\n",
            "Epoch: 12780, loss: 0.00046\n",
            "Epoch: 12790, loss: 0.00046\n",
            "Epoch: 12800, loss: 0.00045\n",
            "Epoch: 12810, loss: 0.00044\n",
            "Epoch: 12820, loss: 0.00043\n",
            "Epoch: 12830, loss: 0.00042\n",
            "Epoch: 12840, loss: 0.00042\n",
            "Epoch: 12850, loss: 0.00041\n",
            "Epoch: 12860, loss: 0.00040\n",
            "Epoch: 12870, loss: 0.00040\n",
            "Epoch: 12880, loss: 0.00039\n",
            "Epoch: 12890, loss: 0.00038\n",
            "Epoch: 12900, loss: 0.00038\n",
            "Epoch: 12910, loss: 0.00040\n",
            "Epoch: 12920, loss: 0.00042\n",
            "Epoch: 12930, loss: 0.00042\n",
            "Epoch: 12940, loss: 0.00041\n",
            "Epoch: 12950, loss: 0.00040\n",
            "Epoch: 12960, loss: 0.00037\n",
            "Epoch: 12970, loss: 0.00036\n",
            "Epoch: 12980, loss: 0.00036\n",
            "Epoch: 12990, loss: 0.00035\n",
            "Epoch: 13000, loss: 0.00034\n",
            "Epoch: 13010, loss: 0.00034\n",
            "Epoch: 13020, loss: 0.00033\n",
            "Epoch: 13030, loss: 0.00033\n",
            "Epoch: 13040, loss: 0.00032\n",
            "Epoch: 13050, loss: 0.00032\n",
            "Epoch: 13060, loss: 0.00031\n",
            "Epoch: 13070, loss: 0.00031\n",
            "Epoch: 13080, loss: 0.00031\n",
            "Epoch: 13090, loss: 0.00030\n",
            "Epoch: 13100, loss: 0.00030\n",
            "Epoch: 13110, loss: 0.00251\n",
            "Epoch: 13120, loss: 0.00031\n",
            "Epoch: 13130, loss: 0.00031\n",
            "Epoch: 13140, loss: 0.00030\n",
            "Epoch: 13150, loss: 0.00030\n",
            "Epoch: 13160, loss: 0.00029\n",
            "Epoch: 13170, loss: 0.00029\n",
            "Epoch: 13180, loss: 0.00029\n",
            "Epoch: 13190, loss: 0.00028\n",
            "Epoch: 13200, loss: 0.00028\n",
            "Epoch: 13210, loss: 0.00027\n",
            "Epoch: 13220, loss: 0.00027\n",
            "Epoch: 13230, loss: 0.00027\n",
            "Epoch: 13240, loss: 0.00026\n",
            "Epoch: 13250, loss: 0.00026\n",
            "Epoch: 13260, loss: 0.00025\n",
            "Epoch: 13270, loss: 0.00025\n",
            "Epoch: 13280, loss: 0.00025\n",
            "Epoch: 13290, loss: 0.00024\n",
            "Epoch: 13300, loss: 0.00024\n",
            "Epoch: 13310, loss: 0.00024\n",
            "Epoch: 13320, loss: 0.00024\n",
            "Epoch: 13330, loss: 0.00023\n",
            "Epoch: 13340, loss: 0.00023\n",
            "Epoch: 13350, loss: 0.00023\n",
            "Epoch: 13360, loss: 0.00022\n",
            "Epoch: 13370, loss: 0.00022\n",
            "Epoch: 13380, loss: 0.00022\n",
            "Epoch: 13390, loss: 0.00022\n",
            "Epoch: 13400, loss: 0.00021\n",
            "Epoch: 13410, loss: 0.00021\n",
            "Epoch: 13420, loss: 0.00021\n",
            "Epoch: 13430, loss: 0.00021\n",
            "Epoch: 13440, loss: 0.00020\n",
            "Epoch: 13450, loss: 0.00020\n",
            "Epoch: 13460, loss: 0.00020\n",
            "Epoch: 13470, loss: 0.00020\n",
            "Epoch: 13480, loss: 0.00020\n",
            "Epoch: 13490, loss: 0.00019\n",
            "Epoch: 13500, loss: 0.00019\n",
            "Epoch: 13510, loss: 0.00019\n",
            "Epoch: 13520, loss: 0.00019\n",
            "Epoch: 13530, loss: 0.00018\n",
            "Epoch: 13540, loss: 0.00018\n",
            "Epoch: 13550, loss: 0.00018\n",
            "Epoch: 13560, loss: 0.00018\n",
            "Epoch: 13570, loss: 0.00018\n",
            "Epoch: 13580, loss: 0.00018\n",
            "Epoch: 13590, loss: 0.00017\n",
            "Epoch: 13600, loss: 0.00018\n",
            "Epoch: 13610, loss: 0.00018\n",
            "Epoch: 13620, loss: 0.00018\n",
            "Epoch: 13630, loss: 0.00018\n",
            "Epoch: 13640, loss: 0.00018\n",
            "Epoch: 13650, loss: 0.00017\n",
            "Epoch: 13660, loss: 0.00017\n",
            "Epoch: 13670, loss: 0.00017\n",
            "Epoch: 13680, loss: 0.00017\n",
            "Epoch: 13690, loss: 0.00017\n",
            "Epoch: 13700, loss: 0.00017\n",
            "Epoch: 13710, loss: 0.00016\n",
            "Epoch: 13720, loss: 0.00016\n",
            "Epoch: 13730, loss: 0.00016\n",
            "Epoch: 13740, loss: 0.00016\n",
            "Epoch: 13750, loss: 0.00016\n",
            "Epoch: 13760, loss: 0.00015\n",
            "Epoch: 13770, loss: 0.00015\n",
            "Epoch: 13780, loss: 0.00466\n",
            "Epoch: 13790, loss: 0.00015\n",
            "Epoch: 13800, loss: 0.00015\n",
            "Epoch: 13810, loss: 0.00015\n",
            "Epoch: 13820, loss: 0.00015\n",
            "Epoch: 13830, loss: 0.00014\n",
            "Epoch: 13840, loss: 0.00014\n",
            "Epoch: 13850, loss: 0.00014\n",
            "Epoch: 13860, loss: 0.00014\n",
            "Epoch: 13870, loss: 0.00014\n",
            "Epoch: 13880, loss: 0.00014\n",
            "Epoch: 13890, loss: 0.00014\n",
            "Epoch: 13900, loss: 0.00014\n",
            "Epoch: 13910, loss: 0.00014\n",
            "Epoch: 13920, loss: 0.00014\n",
            "Epoch: 13930, loss: 0.00014\n",
            "Epoch: 13940, loss: 0.00013\n",
            "Epoch: 13950, loss: 0.00013\n",
            "Epoch: 13960, loss: 0.00013\n",
            "Epoch: 13970, loss: 0.00013\n",
            "Epoch: 13980, loss: 0.00013\n",
            "Epoch: 13990, loss: 0.00013\n",
            "Epoch: 14000, loss: 0.00013\n",
            "Epoch: 14010, loss: 0.00013\n",
            "Epoch: 14020, loss: 0.00012\n",
            "Epoch: 14030, loss: 0.00012\n",
            "Epoch: 14040, loss: 0.00012\n",
            "Epoch: 14050, loss: 0.00012\n",
            "Epoch: 14060, loss: 0.00012\n",
            "Epoch: 14070, loss: 0.00012\n",
            "Epoch: 14080, loss: 0.00012\n",
            "Epoch: 14090, loss: 0.00012\n",
            "Epoch: 14100, loss: 0.00012\n",
            "Epoch: 14110, loss: 0.00011\n",
            "Epoch: 14120, loss: 0.00011\n",
            "Epoch: 14130, loss: 0.00011\n",
            "Epoch: 14140, loss: 0.00011\n",
            "Epoch: 14150, loss: 0.00011\n",
            "Epoch: 14160, loss: 0.00011\n",
            "Epoch: 14170, loss: 0.00011\n",
            "Epoch: 14180, loss: 0.00011\n",
            "Epoch: 14190, loss: 0.00011\n",
            "Epoch: 14200, loss: 0.00011\n",
            "Epoch: 14210, loss: 0.00011\n",
            "Epoch: 14220, loss: 0.00011\n",
            "Epoch: 14230, loss: 0.00010\n",
            "Epoch: 14240, loss: 0.00010\n",
            "Epoch: 14250, loss: 0.00010\n",
            "Epoch: 14260, loss: 0.00010\n",
            "Epoch: 14270, loss: 0.00010\n",
            "Epoch: 14280, loss: 0.00010\n",
            "Epoch: 14290, loss: 0.00010\n",
            "Epoch: 14300, loss: 0.00010\n",
            "Epoch: 14310, loss: 0.00010\n",
            "Epoch: 14320, loss: 0.00010\n",
            "Epoch: 14330, loss: 0.00010\n",
            "Epoch: 14340, loss: 0.00010\n",
            "Epoch: 14350, loss: 0.00009\n",
            "Epoch: 14360, loss: 0.00009\n",
            "Epoch: 14370, loss: 0.00009\n",
            "Epoch: 14380, loss: 0.00009\n",
            "Epoch: 14390, loss: 0.00009\n",
            "Epoch: 14400, loss: 0.00009\n",
            "Epoch: 14410, loss: 0.00009\n",
            "Epoch: 14420, loss: 0.00009\n",
            "Epoch: 14430, loss: 0.00009\n",
            "Epoch: 14440, loss: 0.00009\n",
            "Epoch: 14450, loss: 0.00009\n",
            "Epoch: 14460, loss: 0.00009\n",
            "Epoch: 14470, loss: 0.00009\n",
            "Epoch: 14480, loss: 0.00009\n",
            "Epoch: 14490, loss: 0.00482\n",
            "Epoch: 14500, loss: 0.00008\n",
            "Epoch: 14510, loss: 0.00008\n",
            "Epoch: 14520, loss: 0.00008\n",
            "Epoch: 14530, loss: 0.00008\n",
            "Epoch: 14540, loss: 0.00008\n",
            "Epoch: 14550, loss: 0.00008\n",
            "Epoch: 14560, loss: 0.00008\n",
            "Epoch: 14570, loss: 0.00008\n",
            "Epoch: 14580, loss: 0.00008\n",
            "Epoch: 14590, loss: 0.00008\n",
            "Epoch: 14600, loss: 0.00008\n",
            "Epoch: 14610, loss: 0.00008\n",
            "Epoch: 14620, loss: 0.00008\n",
            "Epoch: 14630, loss: 0.00008\n",
            "Epoch: 14640, loss: 0.00008\n",
            "Epoch: 14650, loss: 0.00008\n",
            "Epoch: 14660, loss: 0.00007\n",
            "Epoch: 14670, loss: 0.00007\n",
            "Epoch: 14680, loss: 0.00007\n",
            "Epoch: 14690, loss: 0.00007\n",
            "Epoch: 14700, loss: 0.00007\n",
            "Epoch: 14710, loss: 0.00007\n",
            "Epoch: 14720, loss: 0.00007\n",
            "Epoch: 14730, loss: 0.00007\n",
            "Epoch: 14740, loss: 0.00007\n",
            "Epoch: 14750, loss: 0.00007\n",
            "Epoch: 14760, loss: 0.00007\n",
            "Epoch: 14770, loss: 0.00007\n",
            "Epoch: 14780, loss: 0.00007\n",
            "Epoch: 14790, loss: 0.00007\n",
            "Epoch: 14800, loss: 0.00007\n",
            "Epoch: 14810, loss: 0.00007\n",
            "Epoch: 14820, loss: 0.00007\n",
            "Epoch: 14830, loss: 0.00007\n",
            "Epoch: 14840, loss: 0.00008\n",
            "Epoch: 14850, loss: 0.00008\n",
            "Epoch: 14860, loss: 0.00008\n",
            "Epoch: 14870, loss: 0.00007\n",
            "Epoch: 14880, loss: 0.00007\n",
            "Epoch: 14890, loss: 0.00007\n",
            "Epoch: 14900, loss: 0.00007\n",
            "Epoch: 14910, loss: 0.00007\n",
            "Epoch: 14920, loss: 0.00007\n",
            "Epoch: 14930, loss: 0.00007\n",
            "Epoch: 14940, loss: 0.00007\n",
            "Epoch: 14950, loss: 0.00007\n",
            "Epoch: 14960, loss: 0.00007\n",
            "Epoch: 14970, loss: 0.00007\n",
            "Epoch: 14980, loss: 0.00007\n",
            "Epoch: 14990, loss: 0.00007\n",
            "Epoch: 15000, loss: 0.00007\n",
            "Epoch: 15010, loss: 0.00007\n",
            "Epoch: 15020, loss: 0.00007\n",
            "Epoch: 15030, loss: 0.00006\n",
            "Epoch: 15040, loss: 0.00006\n",
            "Epoch: 15050, loss: 0.00006\n",
            "Epoch: 15060, loss: 0.00006\n",
            "Epoch: 15070, loss: 0.00006\n",
            "Epoch: 15080, loss: 0.00006\n",
            "Epoch: 15090, loss: 0.00006\n",
            "Epoch: 15100, loss: 0.00006\n",
            "Epoch: 15110, loss: 0.00006\n",
            "Epoch: 15120, loss: 0.00006\n",
            "Epoch: 15130, loss: 0.00006\n",
            "Epoch: 15140, loss: 0.00006\n",
            "Epoch: 15150, loss: 0.00006\n",
            "Epoch: 15160, loss: 0.00006\n",
            "Epoch: 15170, loss: 0.00006\n",
            "Epoch: 15180, loss: 0.00006\n",
            "Epoch: 15190, loss: 0.00006\n",
            "Epoch: 15200, loss: 0.00006\n",
            "Epoch: 15210, loss: 0.00006\n",
            "Epoch: 15220, loss: 0.00006\n",
            "Epoch: 15230, loss: 0.00006\n",
            "Epoch: 15240, loss: 0.00006\n",
            "Epoch: 15250, loss: 0.00006\n",
            "Epoch: 15260, loss: 0.00005\n",
            "Epoch: 15270, loss: 0.00005\n",
            "Epoch: 15280, loss: 0.00005\n",
            "Epoch: 15290, loss: 0.00005\n",
            "Epoch: 15300, loss: 0.00005\n",
            "Epoch: 15310, loss: 0.00005\n",
            "Epoch: 15320, loss: 0.00005\n",
            "Epoch: 15330, loss: 0.00005\n",
            "Epoch: 15340, loss: 0.00005\n",
            "Epoch: 15350, loss: 0.00005\n",
            "Epoch: 15360, loss: 0.00005\n",
            "Epoch: 15370, loss: 0.00005\n",
            "Epoch: 15380, loss: 0.00006\n",
            "Epoch: 15390, loss: 0.00006\n",
            "Epoch: 15400, loss: 0.00006\n",
            "Epoch: 15410, loss: 0.00006\n",
            "Epoch: 15420, loss: 0.00006\n",
            "Epoch: 15430, loss: 0.00006\n",
            "Epoch: 15440, loss: 0.00006\n",
            "Epoch: 15450, loss: 0.00005\n",
            "Epoch: 15460, loss: 0.00005\n",
            "Epoch: 15470, loss: 0.00005\n",
            "Epoch: 15480, loss: 0.00005\n",
            "Epoch: 15490, loss: 0.00005\n",
            "Epoch: 15500, loss: 0.00005\n",
            "Epoch: 15510, loss: 0.00005\n",
            "Epoch: 15520, loss: 0.00005\n",
            "Epoch: 15530, loss: 0.00005\n",
            "Epoch: 15540, loss: 0.00005\n",
            "Epoch: 15550, loss: 0.00005\n",
            "Epoch: 15560, loss: 0.00005\n",
            "Epoch: 15570, loss: 0.00005\n",
            "Epoch: 15580, loss: 0.00005\n",
            "Epoch: 15590, loss: 0.00005\n",
            "Epoch: 15600, loss: 0.00005\n",
            "Epoch: 15610, loss: 0.00005\n",
            "Epoch: 15620, loss: 0.00005\n",
            "Epoch: 15630, loss: 0.00005\n",
            "Epoch: 15640, loss: 0.00005\n",
            "Epoch: 15650, loss: 0.00005\n",
            "Epoch: 15660, loss: 0.00005\n",
            "Epoch: 15670, loss: 0.00005\n",
            "Epoch: 15680, loss: 0.00004\n",
            "Epoch: 15690, loss: 0.00004\n",
            "Epoch: 15700, loss: 0.00004\n",
            "Epoch: 15710, loss: 0.00004\n",
            "Epoch: 15720, loss: 0.00004\n",
            "Epoch: 15730, loss: 0.00004\n",
            "Epoch: 15740, loss: 0.00004\n",
            "Epoch: 15750, loss: 0.00004\n",
            "Epoch: 15760, loss: 0.00004\n",
            "Epoch: 15770, loss: 0.00004\n",
            "Epoch: 15780, loss: 0.00004\n",
            "Epoch: 15790, loss: 0.00004\n",
            "Epoch: 15800, loss: 0.00004\n",
            "Epoch: 15810, loss: 0.00004\n",
            "Epoch: 15820, loss: 0.00004\n",
            "Epoch: 15830, loss: 0.00004\n",
            "Epoch: 15840, loss: 0.00004\n",
            "Epoch: 15850, loss: 0.00004\n",
            "Epoch: 15860, loss: 0.00004\n",
            "Epoch: 15870, loss: 0.00004\n",
            "Epoch: 15880, loss: 0.00004\n",
            "Epoch: 15890, loss: 0.00004\n",
            "Epoch: 15900, loss: 0.00004\n",
            "Epoch: 15910, loss: 0.00004\n",
            "Epoch: 15920, loss: 0.00004\n",
            "Epoch: 15930, loss: 0.00004\n",
            "Epoch: 15940, loss: 0.00004\n",
            "Epoch: 15950, loss: 0.00004\n",
            "Epoch: 15960, loss: 0.00004\n",
            "Epoch: 15970, loss: 0.00004\n",
            "Epoch: 15980, loss: 0.00004\n",
            "Epoch: 15990, loss: 0.00004\n",
            "Epoch: 16000, loss: 0.00004\n",
            "Epoch: 16010, loss: 0.00004\n",
            "Epoch: 16020, loss: 0.00004\n",
            "Epoch: 16030, loss: 0.00003\n",
            "Epoch: 16040, loss: 0.00003\n",
            "Epoch: 16050, loss: 0.00003\n",
            "Epoch: 16060, loss: 0.00003\n",
            "Epoch: 16070, loss: 0.00003\n",
            "Epoch: 16080, loss: 0.00003\n",
            "Epoch: 16090, loss: 0.00003\n",
            "Epoch: 16100, loss: 0.00003\n",
            "Epoch: 16110, loss: 0.00003\n",
            "Epoch: 16120, loss: 0.00003\n",
            "Epoch: 16130, loss: 0.00003\n",
            "Epoch: 16140, loss: 0.00003\n",
            "Epoch: 16150, loss: 0.00003\n",
            "Epoch: 16160, loss: 0.00003\n",
            "Epoch: 16170, loss: 0.00003\n",
            "Epoch: 16180, loss: 0.00003\n",
            "Epoch: 16190, loss: 0.00003\n",
            "Epoch: 16200, loss: 0.00003\n",
            "Epoch: 16210, loss: 0.00003\n",
            "Epoch: 16220, loss: 0.00003\n",
            "Epoch: 16230, loss: 0.00003\n",
            "Epoch: 16240, loss: 0.00003\n",
            "Epoch: 16250, loss: 0.00003\n",
            "Epoch: 16260, loss: 0.00003\n",
            "Epoch: 16270, loss: 0.00003\n",
            "Epoch: 16280, loss: 0.00003\n",
            "Epoch: 16290, loss: 0.00003\n",
            "Epoch: 16300, loss: 0.00003\n",
            "Epoch: 16310, loss: 0.00003\n",
            "Epoch: 16320, loss: 0.00003\n",
            "Epoch: 16330, loss: 0.00003\n",
            "Epoch: 16340, loss: 0.00003\n",
            "Epoch: 16350, loss: 0.00003\n",
            "Epoch: 16360, loss: 0.00003\n",
            "Epoch: 16370, loss: 0.00003\n",
            "Epoch: 16380, loss: 0.00003\n",
            "Epoch: 16390, loss: 0.00003\n",
            "Epoch: 16400, loss: 0.00003\n",
            "Epoch: 16410, loss: 0.00003\n",
            "Epoch: 16420, loss: 0.00003\n",
            "Epoch: 16430, loss: 0.00003\n",
            "Epoch: 16440, loss: 0.00003\n",
            "Epoch: 16450, loss: 0.00003\n",
            "Epoch: 16460, loss: 0.00003\n",
            "Epoch: 16470, loss: 0.00003\n",
            "Epoch: 16480, loss: 0.00003\n",
            "Epoch: 16490, loss: 0.00003\n",
            "Epoch: 16500, loss: 0.00003\n",
            "Epoch: 16510, loss: 0.00003\n",
            "Epoch: 16520, loss: 0.00003\n",
            "Epoch: 16530, loss: 0.00003\n",
            "Epoch: 16540, loss: 0.00003\n",
            "Epoch: 16550, loss: 0.00003\n",
            "Epoch: 16560, loss: 0.00004\n",
            "Epoch: 16570, loss: 0.00003\n",
            "Epoch: 16580, loss: 0.00003\n",
            "Epoch: 16590, loss: 0.00003\n",
            "Epoch: 16600, loss: 0.00003\n",
            "Epoch: 16610, loss: 0.00003\n",
            "Epoch: 16620, loss: 0.00003\n",
            "Epoch: 16630, loss: 0.00003\n",
            "Epoch: 16640, loss: 0.00003\n",
            "Epoch: 16650, loss: 0.00003\n",
            "Epoch: 16660, loss: 0.00003\n",
            "Epoch: 16670, loss: 0.00003\n",
            "Epoch: 16680, loss: 0.00003\n",
            "Epoch: 16690, loss: 0.00003\n",
            "Epoch: 16700, loss: 0.00003\n",
            "Epoch: 16710, loss: 0.00003\n",
            "Epoch: 16720, loss: 0.00003\n",
            "Epoch: 16730, loss: 0.00003\n",
            "Epoch: 16740, loss: 0.00003\n",
            "Epoch: 16750, loss: 0.00003\n",
            "Epoch: 16760, loss: 0.00003\n",
            "Epoch: 16770, loss: 0.00003\n",
            "Epoch: 16780, loss: 0.00003\n",
            "Epoch: 16790, loss: 0.00003\n",
            "Epoch: 16800, loss: 0.00003\n",
            "Epoch: 16810, loss: 0.00003\n",
            "Epoch: 16820, loss: 0.00003\n",
            "Epoch: 16830, loss: 0.00003\n",
            "Epoch: 16840, loss: 0.00003\n",
            "Epoch: 16850, loss: 0.00003\n",
            "Epoch: 16860, loss: 0.00002\n",
            "Epoch: 16870, loss: 0.00002\n",
            "Epoch: 16880, loss: 0.00002\n",
            "Epoch: 16890, loss: 0.00002\n",
            "Epoch: 16900, loss: 0.00002\n",
            "Epoch: 16910, loss: 0.00002\n",
            "Epoch: 16920, loss: 0.00002\n",
            "Epoch: 16930, loss: 0.00002\n",
            "Epoch: 16940, loss: 0.00002\n",
            "Epoch: 16950, loss: 0.00002\n",
            "Epoch: 16960, loss: 0.00002\n",
            "Epoch: 16970, loss: 0.00002\n",
            "Epoch: 16980, loss: 0.00002\n",
            "Epoch: 16990, loss: 0.00002\n",
            "Epoch: 17000, loss: 0.00002\n",
            "Epoch: 17010, loss: 0.00002\n",
            "Epoch: 17020, loss: 0.00002\n",
            "Epoch: 17030, loss: 0.00002\n",
            "Epoch: 17040, loss: 0.00002\n",
            "Epoch: 17050, loss: 0.00002\n",
            "Epoch: 17060, loss: 0.00002\n",
            "Epoch: 17070, loss: 0.00002\n",
            "Epoch: 17080, loss: 0.00002\n",
            "Epoch: 17090, loss: 0.00002\n",
            "Epoch: 17100, loss: 0.00002\n",
            "Epoch: 17110, loss: 0.00002\n",
            "Epoch: 17120, loss: 0.00002\n",
            "Epoch: 17130, loss: 0.00002\n",
            "Epoch: 17140, loss: 0.00002\n",
            "Epoch: 17150, loss: 0.00002\n",
            "Epoch: 17160, loss: 0.00002\n",
            "Epoch: 17170, loss: 0.00002\n",
            "Epoch: 17180, loss: 0.00002\n",
            "Epoch: 17190, loss: 0.00002\n",
            "Epoch: 17200, loss: 0.00002\n",
            "Epoch: 17210, loss: 0.00002\n",
            "Epoch: 17220, loss: 0.00002\n",
            "Epoch: 17230, loss: 0.00002\n",
            "Epoch: 17240, loss: 0.00002\n",
            "Epoch: 17250, loss: 0.00002\n",
            "Epoch: 17260, loss: 0.00002\n",
            "Epoch: 17270, loss: 0.00002\n",
            "Epoch: 17280, loss: 0.00002\n",
            "Epoch: 17290, loss: 0.00002\n",
            "Epoch: 17300, loss: 0.00002\n",
            "Epoch: 17310, loss: 0.00002\n",
            "Epoch: 17320, loss: 0.00002\n",
            "Epoch: 17330, loss: 0.00002\n",
            "Epoch: 17340, loss: 0.00002\n",
            "Epoch: 17350, loss: 0.00002\n",
            "Epoch: 17360, loss: 0.00002\n",
            "Epoch: 17370, loss: 0.00002\n",
            "Epoch: 17380, loss: 0.00002\n",
            "Epoch: 17390, loss: 0.00002\n",
            "Epoch: 17400, loss: 0.00002\n",
            "Epoch: 17410, loss: 0.00002\n",
            "Epoch: 17420, loss: 0.00002\n",
            "Epoch: 17430, loss: 0.00002\n",
            "Epoch: 17440, loss: 0.00002\n",
            "Epoch: 17450, loss: 0.00002\n",
            "Epoch: 17460, loss: 0.00002\n",
            "Epoch: 17470, loss: 0.00002\n",
            "Epoch: 17480, loss: 0.00002\n",
            "Epoch: 17490, loss: 0.00002\n",
            "Epoch: 17500, loss: 0.00002\n",
            "Epoch: 17510, loss: 0.00002\n",
            "Epoch: 17520, loss: 0.00002\n",
            "Epoch: 17530, loss: 0.00002\n",
            "Epoch: 17540, loss: 0.00002\n",
            "Epoch: 17550, loss: 0.00002\n",
            "Epoch: 17560, loss: 0.00002\n",
            "Epoch: 17570, loss: 0.00002\n",
            "Epoch: 17580, loss: 0.00002\n",
            "Epoch: 17590, loss: 0.00002\n",
            "Epoch: 17600, loss: 0.00002\n",
            "Epoch: 17610, loss: 0.00002\n",
            "Epoch: 17620, loss: 0.00002\n",
            "Epoch: 17630, loss: 0.00002\n",
            "Epoch: 17640, loss: 0.00002\n",
            "Epoch: 17650, loss: 0.00001\n",
            "Epoch: 17660, loss: 0.00001\n",
            "Epoch: 17670, loss: 0.00001\n",
            "Epoch: 17680, loss: 0.00001\n",
            "Epoch: 17690, loss: 0.00001\n",
            "Epoch: 17700, loss: 0.00001\n",
            "Epoch: 17710, loss: 0.00001\n",
            "Epoch: 17720, loss: 0.00001\n",
            "Epoch: 17730, loss: 0.00001\n",
            "Epoch: 17740, loss: 0.00001\n",
            "Epoch: 17750, loss: 0.00001\n",
            "Epoch: 17760, loss: 0.00001\n",
            "Epoch: 17770, loss: 0.00001\n",
            "Epoch: 17780, loss: 0.00001\n",
            "Epoch: 17790, loss: 0.00001\n",
            "Epoch: 17800, loss: 0.00001\n",
            "Epoch: 17810, loss: 0.00001\n",
            "Epoch: 17820, loss: 0.00001\n",
            "Epoch: 17830, loss: 0.00001\n",
            "Epoch: 17840, loss: 0.00001\n",
            "Epoch: 17850, loss: 0.00001\n",
            "Epoch: 17860, loss: 0.00001\n",
            "Epoch: 17870, loss: 0.00001\n",
            "Epoch: 17880, loss: 0.00001\n",
            "Epoch: 17890, loss: 0.00001\n",
            "Epoch: 17900, loss: 0.00001\n",
            "Epoch: 17910, loss: 0.00001\n",
            "Epoch: 17920, loss: 0.00001\n",
            "Epoch: 17930, loss: 0.00001\n",
            "Epoch: 17940, loss: 0.00001\n",
            "Epoch: 17950, loss: 0.00001\n",
            "Epoch: 17960, loss: 0.00001\n",
            "Epoch: 17970, loss: 0.00001\n",
            "Epoch: 17980, loss: 0.00001\n",
            "Epoch: 17990, loss: 0.00001\n",
            "Epoch: 18000, loss: 0.00001\n",
            "Epoch: 18010, loss: 0.00001\n",
            "Epoch: 18020, loss: 0.00001\n",
            "Epoch: 18030, loss: 0.00001\n",
            "Epoch: 18040, loss: 0.00001\n",
            "Epoch: 18050, loss: 0.00001\n",
            "Epoch: 18060, loss: 0.00001\n",
            "Epoch: 18070, loss: 0.00001\n",
            "Epoch: 18080, loss: 0.00001\n",
            "Epoch: 18090, loss: 0.00001\n",
            "Epoch: 18100, loss: 0.00001\n",
            "Epoch: 18110, loss: 0.00001\n",
            "Epoch: 18120, loss: 0.00001\n",
            "Epoch: 18130, loss: 0.00001\n",
            "Epoch: 18140, loss: 0.00001\n",
            "Epoch: 18150, loss: 0.00001\n",
            "Epoch: 18160, loss: 0.00001\n",
            "Epoch: 18170, loss: 0.00001\n",
            "Epoch: 18180, loss: 0.00001\n",
            "Epoch: 18190, loss: 0.00001\n",
            "Epoch: 18200, loss: 0.00001\n",
            "Epoch: 18210, loss: 0.00001\n",
            "Epoch: 18220, loss: 0.00001\n",
            "Epoch: 18230, loss: 0.00001\n",
            "Epoch: 18240, loss: 0.00001\n",
            "Epoch: 18250, loss: 0.00001\n",
            "Epoch: 18260, loss: 0.00001\n",
            "Epoch: 18270, loss: 0.00001\n",
            "Epoch: 18280, loss: 0.00001\n",
            "Epoch: 18290, loss: 0.00001\n",
            "Epoch: 18300, loss: 0.00001\n",
            "Epoch: 18310, loss: 0.00001\n",
            "Epoch: 18320, loss: 0.00001\n",
            "Epoch: 18330, loss: 0.00001\n",
            "Epoch: 18340, loss: 0.00001\n",
            "Epoch: 18350, loss: 0.00001\n",
            "Epoch: 18360, loss: 0.00001\n",
            "Epoch: 18370, loss: 0.00001\n",
            "Epoch: 18380, loss: 0.00001\n",
            "Epoch: 18390, loss: 0.00001\n",
            "Epoch: 18400, loss: 0.00001\n",
            "Epoch: 18410, loss: 0.00001\n",
            "Epoch: 18420, loss: 0.00001\n",
            "Epoch: 18430, loss: 0.00001\n",
            "Epoch: 18440, loss: 0.00001\n",
            "Epoch: 18450, loss: 0.00001\n",
            "Epoch: 18460, loss: 0.00001\n",
            "Epoch: 18470, loss: 0.00001\n",
            "Epoch: 18480, loss: 0.00001\n",
            "Epoch: 18490, loss: 0.00001\n",
            "Epoch: 18500, loss: 0.00001\n",
            "Epoch: 18510, loss: 0.00001\n",
            "Epoch: 18520, loss: 0.00001\n",
            "Epoch: 18530, loss: 0.00001\n",
            "Epoch: 18540, loss: 0.00001\n",
            "Epoch: 18550, loss: 0.00001\n",
            "Epoch: 18560, loss: 0.00001\n",
            "Epoch: 18570, loss: 0.00001\n",
            "Epoch: 18580, loss: 0.00001\n",
            "Epoch: 18590, loss: 0.00001\n",
            "Epoch: 18600, loss: 0.00001\n",
            "Epoch: 18610, loss: 0.00001\n",
            "Epoch: 18620, loss: 0.00001\n",
            "Epoch: 18630, loss: 0.00001\n",
            "Epoch: 18640, loss: 0.00001\n",
            "Epoch: 18650, loss: 0.00001\n",
            "Epoch: 18660, loss: 0.00001\n",
            "Epoch: 18670, loss: 0.00001\n",
            "Epoch: 18680, loss: 0.00001\n",
            "Epoch: 18690, loss: 0.00001\n",
            "Epoch: 18700, loss: 0.00001\n",
            "Epoch: 18710, loss: 0.00001\n",
            "Epoch: 18720, loss: 0.00001\n",
            "Epoch: 18730, loss: 0.00001\n",
            "Epoch: 18740, loss: 0.00001\n",
            "Epoch: 18750, loss: 0.00001\n",
            "Epoch: 18760, loss: 0.00001\n",
            "Epoch: 18770, loss: 0.00001\n",
            "Epoch: 18780, loss: 0.00001\n",
            "Epoch: 18790, loss: 0.00001\n",
            "Epoch: 18800, loss: 0.00001\n",
            "Epoch: 18810, loss: 0.00001\n",
            "Epoch: 18820, loss: 0.00001\n",
            "Epoch: 18830, loss: 0.00001\n",
            "Epoch: 18840, loss: 0.00001\n",
            "Epoch: 18850, loss: 0.00001\n",
            "Epoch: 18860, loss: 0.00001\n",
            "Epoch: 18870, loss: 0.00001\n",
            "Epoch: 18880, loss: 0.00001\n",
            "Epoch: 18890, loss: 0.00001\n",
            "Epoch: 18900, loss: 0.00001\n",
            "Epoch: 18910, loss: 0.00001\n",
            "Epoch: 18920, loss: 0.00001\n",
            "Epoch: 18930, loss: 0.00001\n",
            "Epoch: 18940, loss: 0.00001\n",
            "Epoch: 18950, loss: 0.00001\n",
            "Epoch: 18960, loss: 0.00001\n",
            "Epoch: 18970, loss: 0.00001\n",
            "Epoch: 18980, loss: 0.00001\n",
            "Epoch: 18990, loss: 0.00001\n",
            "Epoch: 19000, loss: 0.00001\n",
            "Epoch: 19010, loss: 0.00001\n",
            "Epoch: 19020, loss: 0.00001\n",
            "Epoch: 19030, loss: 0.00001\n",
            "Epoch: 19040, loss: 0.00001\n",
            "Epoch: 19050, loss: 0.00001\n",
            "Epoch: 19060, loss: 0.00001\n",
            "Epoch: 19070, loss: 0.00001\n",
            "Epoch: 19080, loss: 0.00001\n",
            "Epoch: 19090, loss: 0.00001\n",
            "Epoch: 19100, loss: 0.00001\n",
            "Epoch: 19110, loss: 0.00001\n",
            "Epoch: 19120, loss: 0.00001\n",
            "Epoch: 19130, loss: 0.00001\n",
            "Epoch: 19140, loss: 0.00001\n",
            "Epoch: 19150, loss: 0.00001\n",
            "Epoch: 19160, loss: 0.00001\n",
            "Epoch: 19170, loss: 0.00001\n",
            "Epoch: 19180, loss: 0.00001\n",
            "Epoch: 19190, loss: 0.00001\n",
            "Epoch: 19200, loss: 0.00001\n",
            "Epoch: 19210, loss: 0.00001\n",
            "Epoch: 19220, loss: 0.00001\n",
            "Epoch: 19230, loss: 0.00001\n",
            "Epoch: 19240, loss: 0.00001\n",
            "Epoch: 19250, loss: 0.00001\n",
            "Epoch: 19260, loss: 0.00001\n",
            "Epoch: 19270, loss: 0.00001\n",
            "Epoch: 19280, loss: 0.00001\n",
            "Epoch: 19290, loss: 0.00001\n",
            "Epoch: 19300, loss: 0.00001\n",
            "Epoch: 19310, loss: 0.00001\n",
            "Epoch: 19320, loss: 0.00001\n",
            "Epoch: 19330, loss: 0.00001\n",
            "Epoch: 19340, loss: 0.00001\n",
            "Epoch: 19350, loss: 0.00001\n",
            "Epoch: 19360, loss: 0.00001\n",
            "Epoch: 19370, loss: 0.00001\n",
            "Epoch: 19380, loss: 0.00001\n",
            "Epoch: 19390, loss: 0.00001\n",
            "Epoch: 19400, loss: 0.00001\n",
            "Epoch: 19410, loss: 0.00001\n",
            "Epoch: 19420, loss: 0.00001\n",
            "Epoch: 19430, loss: 0.00001\n",
            "Epoch: 19440, loss: 0.00001\n",
            "Epoch: 19450, loss: 0.00001\n",
            "Epoch: 19460, loss: 0.00001\n",
            "Epoch: 19470, loss: 0.00001\n",
            "Epoch: 19480, loss: 0.00001\n",
            "Epoch: 19490, loss: 0.00001\n",
            "Epoch: 19500, loss: 0.00001\n",
            "Epoch: 19510, loss: 0.00001\n",
            "Epoch: 19520, loss: 0.00001\n",
            "Epoch: 19530, loss: 0.00001\n",
            "Epoch: 19540, loss: 0.00001\n",
            "Epoch: 19550, loss: 0.00001\n",
            "Epoch: 19560, loss: 0.00001\n",
            "Epoch: 19570, loss: 0.00000\n",
            "Epoch: 19580, loss: 0.00000\n",
            "Epoch: 19590, loss: 0.00000\n",
            "Epoch: 19600, loss: 0.00000\n",
            "Epoch: 19610, loss: 0.00000\n",
            "Epoch: 19620, loss: 0.00000\n",
            "Epoch: 19630, loss: 0.00000\n",
            "Epoch: 19640, loss: 0.00000\n",
            "Epoch: 19650, loss: 0.00000\n",
            "Epoch: 19660, loss: 0.00000\n",
            "Epoch: 19670, loss: 0.00000\n",
            "Epoch: 19680, loss: 0.00000\n",
            "Epoch: 19690, loss: 0.00000\n",
            "Epoch: 19700, loss: 0.00000\n",
            "Epoch: 19710, loss: 0.00000\n",
            "Epoch: 19720, loss: 0.00000\n",
            "Epoch: 19730, loss: 0.00000\n",
            "Epoch: 19740, loss: 0.00000\n",
            "Epoch: 19750, loss: 0.00000\n",
            "Epoch: 19760, loss: 0.00000\n",
            "Epoch: 19770, loss: 0.00000\n",
            "Epoch: 19780, loss: 0.00000\n",
            "Epoch: 19790, loss: 0.00000\n",
            "Epoch: 19800, loss: 0.00000\n",
            "Epoch: 19810, loss: 0.00000\n",
            "Epoch: 19820, loss: 0.00000\n",
            "Epoch: 19830, loss: 0.00000\n",
            "Epoch: 19840, loss: 0.00000\n",
            "Epoch: 19850, loss: 0.00000\n",
            "Epoch: 19860, loss: 0.00000\n",
            "Epoch: 19870, loss: 0.00000\n",
            "Epoch: 19880, loss: 0.00000\n",
            "Epoch: 19890, loss: 0.00000\n",
            "Epoch: 19900, loss: 0.00000\n",
            "Epoch: 19910, loss: 0.00000\n",
            "Epoch: 19920, loss: 0.00000\n",
            "Epoch: 19930, loss: 0.00000\n",
            "Epoch: 19940, loss: 0.00000\n",
            "Epoch: 19950, loss: 0.00000\n",
            "Epoch: 19960, loss: 0.00000\n",
            "Epoch: 19970, loss: 0.00000\n",
            "Epoch: 19980, loss: 0.00000\n",
            "Epoch: 19990, loss: 0.00000\n",
            "torch.Size([2000, 11, 11])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define the Hyperparameters\n",
        "n_hidden_layers = 2\n",
        "input_size = p+1\n",
        "hidden_size = p+1\n",
        "output_size = p+1\n",
        "num_epochs = 20000\n",
        "\n",
        "# Define the RNN Class\n",
        "class C_RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, p):\n",
        "        super(C_RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.p = p\n",
        "\n",
        "        # Define the RNN Layer\n",
        "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.smax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    def forward(self, x, train = True):\n",
        "        outputs = []\n",
        "        hidden_states = []\n",
        "        self.initial_hidden = torch.rand(self.num_layers,x.shape[0],self.hidden_size)\n",
        "        for timestep in range(self.p):\n",
        "            h_t = []\n",
        "            if(timestep== 0):\n",
        "                h_t = self.rnn_cell(x[:,timestep], self.initial_hidden[0])\n",
        "            else:\n",
        "                input = []\n",
        "                if(train):\n",
        "                    input = x[:,timestep]\n",
        "                else:\n",
        "                    input = torch.zeros_like(x[:,0])\n",
        "                    temp = torch.argmax(outputs[-1],dim=1)\n",
        "                    for row in range(input.shape[0]):\n",
        "                        input[row][temp[row]] = 1.0\n",
        "                h_t = self.rnn_cell(input, hidden_states[-1])\n",
        "            hidden_states.append(h_t)\n",
        "            if(timestep < self.p -1):\n",
        "                out = self.smax(self.fc(h_t))\n",
        "            else:\n",
        "                out = self.smax(self.fc(h_t))\n",
        "            outputs.append(out)\n",
        "        return outputs[-1]\n",
        "    \n",
        "    def predict(self, x):\n",
        "        return self.forward(x, False)\n",
        "\n",
        "# Initialize the Model\n",
        "model_RNN = C_RNN(input_size, hidden_size, output_size, 1, p)\n",
        "# Run on the GPU\n",
        "model_RNN.to(device)\n",
        "\n",
        "# Define the Loss Function and the Optimizer\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = optim.Adam(model_RNN.parameters(), lr=0.001)\n",
        "\n",
        "loss_list = []\n",
        "test_loss_act = []\n",
        "test_loss_pred = []\n",
        "\n",
        "# Move Test tensor to the same device as the model\n",
        "Test = Test.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    # Here, now you can call the Model\n",
        "    output = model_RNN(Train)  # Pass the entire sequence to the model\n",
        "    # Calculate the loss for the Final Term only\n",
        "    loss = criterion(output, Train[:, -1, :]) # Last term in both\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_list.append(loss.item())\n",
        "    # test_loss_act.append(Loss_Test_Act(Test, model))\n",
        "    # test_loss_pred.append(Loss_Test_Pred(Test, model))\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
        "\n",
        "print(Train.shape)\n",
        "\n",
        "# Save the Model\n",
        "torch.save(model_RNN.state_dict(), \"model.ckpt\")\n",
        "\n",
        "# Plot the Loss Curve\n",
        "plt.plot(loss_list)\n",
        "\n",
        "# Assuming test_loss_act and test_loss_pred are tensors on the GPU\n",
        "# Move them to the CPU before converting to NumPy arrays\n",
        "test_loss_act_cpu = [tl.detach().cpu().numpy() for tl in test_loss_act]\n",
        "test_loss_pred_cpu = [tl.detach().cpu().numpy() for tl in test_loss_pred]\n",
        "\n",
        "plt.plot(test_loss_act_cpu, color='red')\n",
        "plt.plot(test_loss_pred_cpu, color='green')\n",
        "\n",
        "# The rest of your code...\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n"
          ]
        }
      ],
      "source": [
        "# Run the Predict Function on one test element\n",
        "# model.predict(0)\n",
        "i = 32\n",
        "print(Test[i, 0, :])\n",
        "pred = model_RNN.predict(Test[i, :, :].unsqueeze(0))\n",
        "# Find the Maximum of the pred\n",
        "argmax = torch.argmax(pred, dim=1)\n",
        "final_result = torch.zeros_like(pred)\n",
        "final_result[0][argmax] = 1.0\n",
        "print(final_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now, The Evaluation Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the LSTM on the Test Samples: 0.999250\n",
            "Accuracy of the RNN on the Test Samples: 0.978250\n",
            "Accuracy of the LSTM on the Test Samples: 0.999250\n",
            "Accuracy of the RNN on the Test Samples: 0.976250\n",
            "Accuracy of the LSTM on the Test Samples: 0.999250\n",
            "Accuracy of the RNN on the Test Samples: 0.978250\n",
            "Accuracy of the LSTM on the Test Samples: 0.999250\n",
            "Accuracy of the RNN on the Test Samples: 0.980250\n",
            "Accuracy of the LSTM on the Test Samples: 0.999000\n",
            "Accuracy of the RNN on the Test Samples: 0.975500\n",
            "Accuracy of the LSTM on the Test Samples: 0.999500\n",
            "Accuracy of the RNN on the Test Samples: 0.977500\n",
            "Accuracy of the LSTM on the Test Samples: 0.999000\n",
            "Accuracy of the RNN on the Test Samples: 0.978000\n",
            "Accuracy of the LSTM on the Test Samples: 0.999000\n",
            "Accuracy of the RNN on the Test Samples: 0.982250\n",
            "Accuracy of the LSTM on the Test Samples: 0.999500\n",
            "Accuracy of the RNN on the Test Samples: 0.978250\n",
            "Accuracy of the LSTM on the Test Samples: 0.998500\n",
            "Accuracy of the RNN on the Test Samples: 0.974500\n",
            "-------------------------------------------------------------------------------------\n",
            "Average Accuracy of the LSTM on the Test Datasets: 0.999150\n",
            "Average Accuracy of the RNN on the Test Datasets: 0.977900\n"
          ]
        }
      ],
      "source": [
        "def evaluate_models(model_LSTM, model_RNN, Test):\n",
        "    # Evaluate the LSTM\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    LSTM_Output = model_LSTM.predict(Test)\n",
        "\n",
        "    # Find the Argmax of the Output, as above\n",
        "    argmax = torch.argmax(LSTM_Output, dim=1)\n",
        "\n",
        "    # Find the number of correct predictions\n",
        "    correct = (argmax == Test[:, -1, :].argmax(dim=1)).sum().item()\n",
        "    total = Test.shape[0]\n",
        "\n",
        "    LSTM_Stat = (correct/total)\n",
        "\n",
        "    print(\"Accuracy of the LSTM on the Test Samples: %f\" % (correct/total))\n",
        "\n",
        "    # Evaluate the RNN\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    RNN_Output = model_RNN.predict(Test)\n",
        "\n",
        "    # Find the Argmax of the Output, as above\n",
        "    argmax = torch.argmax(RNN_Output, dim=1)\n",
        "\n",
        "    # Find the number of correct predictions\n",
        "    correct = (argmax == Test[:, -1, :].argmax(dim=1)).sum().item()\n",
        "    total = Test.shape[0]\n",
        "    print(\"Accuracy of the RNN on the Test Samples: %f\" % (correct/total))\n",
        "\n",
        "    RNN_Stat = (correct/total)\n",
        "    return LSTM_Stat, RNN_Stat\n",
        "\n",
        "# Generate Multiple Test Datasets similar to how generated above\n",
        "Datasets_Test = []\n",
        "for i in range(10):\n",
        "    Dataset_x, Dataset_y = GenerateDataset(10, 3000)\n",
        "    Test_x = Dataset_x[1000:]\n",
        "    Test_y = Dataset_y[1000:]\n",
        "\n",
        "    # Convert the Dataset into PyTorch Tensors\n",
        "    Test_x = torch.Tensor(Test_x)\n",
        "    Test_y = torch.Tensor(Test_y)\n",
        "\n",
        "    Datasets_Test.append(torch.cat((Test_x, Test_y), 0))\n",
        "\n",
        "# Move the Test Datasets to the same device as the model\n",
        "for i in range(len(Datasets_Test)):\n",
        "    Datasets_Test[i] = Datasets_Test[i].to(device)\n",
        "\n",
        "# Evaluate the Models on the Test Datasets\n",
        "LSTM_Stats = []\n",
        "RNN_Stats = []\n",
        "\n",
        "for i in range(len(Datasets_Test)):\n",
        "    LSTM_Stat, RNN_Stat = evaluate_models(model_LSTM, model_RNN, Datasets_Test[i])\n",
        "    LSTM_Stats.append(LSTM_Stat)\n",
        "    RNN_Stats.append(RNN_Stat)\n",
        "\n",
        "print(\"-------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"Average Accuracy of the LSTM on the Test Datasets: %f\" % (sum(LSTM_Stats)/len(LSTM_Stats)))\n",
        "print(\"Average Accuracy of the RNN on the Test Datasets: %f\" % (sum(RNN_Stats)/len(RNN_Stats)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
